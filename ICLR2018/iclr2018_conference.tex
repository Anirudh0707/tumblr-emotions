\documentclass{article} % For LaTeX2e
\usepackage{iclr2018_conference,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
%% To use subfigures
\usepackage{subcaption}
%% To place figures where you want to
\usepackage{float}
%% To remove whitespace before enumerate
\usepackage{enumitem}
%% For thicker table horizontal lines
\usepackage{booktabs}

\title{Multimodal Sentiment Analysis To Explore the Structure of Emotions}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Anthony Hu \\
Department of Statistics \\
University of Oxford \\
Oxford, United Kingdom \\
\texttt{anthony.hu@stats.ox.ac.uk} \\
\And
Seth Flaxman \\
Department of Mathematics \\
Imperial College \\
London, United Kingdom \\
\texttt{s.flaxman@imperial.ac.uk}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.

\begin{document}

\maketitle

\begin{abstract}
We propose a novel approach to multimodal sentiment analysis using deep neural networks  combining visual recognition and natural language processing. Our goal is different than the standard sentiment analysis goal of predicting whether a sentence expresses positive or negative sentiment; instead, we aim to infer the latent emotional state of the user. Thus, we focus on predicting the emotion word tags attached by users to their Tumblr posts, treating these as ``self-reported emotions.''  We demonstrate that our multimodal model combining both text and image features outperforms separate models based solely on either images or text. Our model's results are interpretable, automatically yielding sensible word lists associated with emotions. We explore the structure of emotions implied by our model and compare it to what has been posited in the psychology literature, and validate our model on a set of images that have been used in psychology studies. Finally, our work also provides a useful tool for the growing academic study of images---both photographs and memes---on social networks. 
\end{abstract}

\section{Introduction}
Sentiment analysis has been an active area of research in the past decade, especially on textual data from Twitter, e.g.~early work by \citet{pak2010twitter} showed that emoticons could be used to collect a labeled dataset for sentiment analysis, \citet{golder2011diurnal} investigated temporal patterns in emotion using tweets, and \citet{Bollen} investigated the impact of collective mood states on the stock market. The SemEval series of ``Sentiment Analysis in Twitter'' challenges has used Twitter data as a benchmark to spur the development of new sentiment analysis algorithms \citep{rosenthal2017semeval}.

Unlike Twitter, Tumblr posts are not limited to 140 characters, allowing more expressiveness, and they often focus on visual content: most Tumblr posts contain an image with some accompanying text. As pictures---both photographs and memes---have become prevalent on social media, researchers have begun to study them, and make novel claims about the role that they play in social media.  \citet{shifman2014memes} makes an argument for taking memes seriously, and  \citet{miller2017visualising} use memes throughout their cross-country  anthropological study of Facebook, characterizing who posts what sorts of memes and what sort of communicative function memes play. Inspired by this research, we take images seriously as a source of data worth analyzing. Further, we aim to enable a research agenda focused on images by giving social scientists tools to address fundamental questions about the use of images on social media. 

Sentiment analysis on text is a well-developed research area in both computer science and psychology, and sentiment analysis has been used to answer psychological questions. However, researchers have cautioned that sentiment analysis focuses on the positive or negative sentiment expressed by a piece of text, rather than on the underlying emotional state of the person who wrote the text \citep{Flaxman-15} and thus is not necessarily a reliable measure of latent emotion. 

In psychology, the gold standard for measuring emotions is self-report, i.e.~if an individual says that they are happy then that is taken to be the truth \citep{gilbert2006stumbling}. On Tumblr, users often attach tags to their posts which we consider to be emotional self-reports, as these tags take the simple form of, e.g.~``\#happy''. By collecting a large dataset and using these emotion word tags as labels, we argue that our sentiment analysis approach, which combines images and text, leads to a more psychologically plausible model, as the dataset combines two rich sources of information and has labels we believe are a good proxy for self-reported emotion. 

Concretely, the Deep Sentiment model associates the features learned by the two modalities as follows:
\begin{itemize}
    \item For images, we  fine-tune Inception \citep{Szegedy-15}, a pre-trained deep convolutional neural network, to our specific task of emotion inferring.
    \item The text is mapped into a rich high-dimensional space using a word representation learned by GloVe \citep{Pennington-14}. The embedded vectors are then fed to a recurrent network which preserves the word order and captures some of the semantics of human language.
    \item A dense layer combines the information in the two modalities and a final softmax output layer gives the probability distribution over the possible emotion word tags.
\end{itemize}

\section{Data from Tumblr}
\label{tumblr-data}

Tumblr is a microblogging service where users post multimedia content that often contains the following attributes: an image, text, and tags. The critical piece of our approach, which distinguishes it from sentiment analysis methods focused on purely distinguishing positive from negative, is that we use the emotion word tags as the labels we wish to predict. As we consider these emotion word tags to be (noisy) labels indicating the user's state of mind when writing a post, we can use them as a proxy for self-reported emotion, and thus a proxy for the underlying emotional state of the user. 

To build our dataset, queries were made through the Tumblr API searching for an emotion appearing in the tags. The 15 emotions retained were those with high relative frequencies on Tumblr among the PANAS-X scale \citep{PANAS-X}. In some posts, the tag containing the emotion of the post also appeared in the text itself. We removed these words from the text in order not to give our classifier an unfair advantage. We extracted every tagged posts from Tumblr by going backward from 2017 until 2011. Due to the API limitations, posts from high density emotions, such as `happy' or `angry', did not go as far back in the past, but otherwise we believe that our dataset is a complete sample.

As Tumblr is used worldwide, we had to filter out non-English posts: posts with less than 50\% of English words were removed from the dataset. The vocabulary of English words was obtained from GloVe (although GloVe's vocabulary contains some non-English words this approach filtered out non-English posts reasonably well). Also, not every extracted post contained an image and we likewise excluded these. Figure \ref{fig:emotions} shows two posts with their associated emotions and table \ref{tumblrdata} summarises the statistics of the data\footnote{We cannot redistribute our dataset due to licensing restrictions, but we will provide code to replicate the dataset on \texttt{https://github.com/deepsentiment/deepsentiment}}.

\begin{figure}[H]
    \begin{subfigure}[t]{.48\textwidth}
        \vskip 0pt %Necessary to align on image and not caption
        \centering
        \includegraphics[height=1.25in]{Images/optimistic.jpg}
        \caption{\textbf{Optimistic}: ``The most beautiful thing we can experience is the mysterious. It is the source of all true art and science -- Albert Einstein.''}
   \end{subfigure}~~
   \begin{subfigure}[t]{.48\textwidth}
       \vskip 0pt
       \centering
       \includegraphics[height=1.25in]{Images/sad.jpg}
       \caption{\textbf{Sad}: ``It's okay to be upset. It's okay to not always be happy. It's okay to cry. Never hide your emotions in fear of upsetting others or of being a bother.''}
    \end{subfigure}
    \caption{Examples of Tumblr posts}
    \label{fig:emotions}
\end{figure}

\begin{table}[H]
\caption{Summary statistics for the Tumblr dataset, with posts from January 2011 to September 2017.}
\begin{center}
    \begin{tabular}{ccc}
    \multicolumn{3}{c}{{\large \textbf{Tumblr data}}} \\
    \addlinespace[0.2cm]
    \toprule
    Posts & English text & English text and image \\
    \midrule
    1,009,534 & 578,699 & 256,897 \\
    \bottomrule
    \end{tabular}
	
    \vspace{0.2cm}
    
    \begin{tabular}{lrrr}
    \toprule
    Emotion & Posts & English text & English text and image\\
    \midrule
\textbf{Happy} & 189,841 & 62\% & 29\% \\
\textbf{Calm} & 139,911 & 37\% & 29\% \\
\textbf{Sad} & 124,900 & 53\% & 15\% \\
\textbf{Scared} & 104,161 & 65\% & 20\% \\
\textbf{Bored} & 101,856 & 54\% & 29\% \\
\textbf{Angry} & 100,033 & 60\% & 21\% \\
\textbf{Annoyed} & 72,993 & 78\% & 10\% \\
\textbf{Love} & 66,146 & 61\% & 39\% \\
\textbf{Excited} & 37,240 & 58\% & 41\% \\
\textbf{Surprised} & 18,322 & 47\% & 32\% \\
\textbf{Optimistic} & 16,111 & 64\% & 36\% \\
\textbf{Amazed} & 10,367 & 61\% & 35\% \\
\textbf{Ashamed} & 10,066 & 63\% & 22\% \\
\textbf{Disgusted} & 9,178 & 69\% & 17\% \\
\textbf{Pensive} & 8,409 & 57\% & 34\% \\
    \bottomrule
    \end{tabular}
    
    %\vspace{0.2cm}
    
    %\begin{tabular}{ccc}
    %\toprule
    %\multicolumn{3}{c}{Text statistics (nb of words)} \\
    %\midrule
    %Min & Mean & Max \\
    %5 & 23 & 11310 \\
    %\bottomrule
    %\end{tabular}
\end{center}
\label{tumblrdata}
\end{table}

\section{Visual analysis}
Images are a valuable source of information for accurately inferring emotional states as they have become ubiquitous on social media as a means for users to express themselves \citep{miller2017visualising}. Although huge progress has been made on standard image classification tasks thanks to the ImageNet challenge \citep{Imagenet-15}, visual sentiment analysis may be fundamentally different from classifying images as it requires a higher level of abstraction to understand the message conveyed by an image \citep{Joshi-11}. Implicit knowledge linked to culture and intrinsic human subjectivity -- that can make two people use the same image to express different emotions -- make visual sentiment analysis a difficult task.

Inspired by the work of \citet{You-15} on visual sentiment analysis using convolutional neural networks (CNNs) on Flickr with domain transfer from Twitter for binary sentiment classification (positive or negative), we tackle image analysis with deep CNNs on a novel (and we argue), richer dataset with the goal of classifying a larger set of emotions.

\subsection{Transfer learning}
Training a convolutional network from scratch can be challenging as a large amount of data is needed and many different architectures have to be tried before achieving satisfying performances. To circumvent this issue, we can take advantage of a pre-trained network named Inception \citep{Szegedy-15} that learned to recognise images through the ImageNet dataset with a deep architecture of 22 layers.

Inception learned representations capturing the colors and arrangement of shapes of an image, which turn out to be relevant when dealing with images even for a different task. We could also say that the pre-trained network grasped the underlying structure of images. This statement rests on the hypothesis that all images lie in a low-dimensional manifold, and recent advances in realistic photos generation through generative adversarial networks bolsters this idea \citep{Radford-16}. 

\subsection{Evaluation}
\label{section:image-evaluation}
To understand how informative the images are, we evaluated the Inception model by fine-tuning it through the last Inception module. After the preprocessing described in Section \ref{tumblr-data}, the dataset contains 256,897 posts that we split into a train set (80\%) and a test set (20\%). The metric used to evaluate the model is accuracy, which is the fraction of correctly classified images.

The Image Sentiment Model is compared to a baseline: random guessing that includes the prior probabilities of the classes, as shown in table \ref{image-results}.

\begin{table}[H]
\caption{Image Sentiment Model and random guessing comparison. The image model was fed raw images that were resized to $(224, 224, 3)$ and fine-tuned through the last Inception module using mini-batches of size 64, Adam optimizer with an initial learning rate of 0.001 and a learning rate decay of 0.3 every epoch. The network minimised the cross-entropy loss and the reported accuracies are from the model trained for 15,000 steps.}
\begin{center}
    \begin{tabular}{ l | c | c}
    & \textbf{Train} & \textbf{Test} \\
    & \textbf{accuracy} & \textbf{accuracy} \\ \hline
    \textbf{Random guessing} & 11\% & 11\% \\ \hline
    \textbf{Image Sentiment Model}  & 43\% & 36\% \\
    \end{tabular}
\end{center}
\label{image-results}
\end{table}

\section{Natural language processing}
Even as a human being, it can be difficult to guess the expressed emotion only by looking at a Tumblr image without reading its caption as shown by Figure \ref{surprised-unclear}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.2\textwidth]{Images/flower.jpg}
    \caption{Which emotion is it?}
    \label{surprised-unclear}
\end{figure}

It is unclear whether the user wants to convey happiness or surprise. Only after reading the accompanying text, ``To whoever left this on my windshield outside of last night's art opening, I love you. You made my night,'' can we finally conclude that the person was {\em surprised} (and possibly also feeling other emotions like amazed). The text is extremely informative and is usually crucial to accurately infer the emotional state.

\subsection{Word embedding}
Most learning algorithms rely on the local smoothness hypothesis, that is, similar training instances are spatially close. This hypothesis clearly doesn't hold with words one-hot encoded as for instance `dog' is as close to `tree' as it is to `cat'. Ideally, we would like to transform the word `dog' into a space in which it is closer to `cat' than it is to `tree'. Word embeddings produce a mapping in which words are projected into a high-dimensional space that preserves semantic relationships. We use the GloVe \citep{Pennington-14} word embedding that was trained on Twitter data, because the writing styles on Twitter and Tumblr are similar.

Each post in the dataset does not necessarily contain the same number of words. Even after embedding each word, the input will be of variable size and most learning algorithm expect a fixed-sized input. We could simply average across the  high-dimensional representations of the words and take a distribution regression approach \citep{muandet2017kernel}. However note that averaging would mean that the word order would be completely lost. Human language relies heavily on word order to communicate as for example the word {\em change} can be both a noun and a verb, and negation such as `not entertained' can only be understood if `not' directly precedes the verb. We will preserve word order by using recurrent neural networks.

\subsection{Sequence input}
Models of natural language using neural networks have proved to outperform the more traditional statistical models that were limited by the Markov assumption \citep{Bengio-03,Goodman-01}. One explanation could be that the compact representation of words through word embeddings is robust \citep{Mikolov-11} and do not need any smoothing over probabilities. Among the neural models, the recurrent-based models allows for short-term memory inspired by how humans read sentences: past context is essential to understand the meaning of written language. Contrary to shallow feedforward networks, that can only cluster similar words, recurrent networks (which can be viewed as a deep architecture \citep{Bengio-07}) can cluster similar histories. 
Recurrent neural networks have a temporal awareness represented by the hidden state that can be seen as an embedding of the past words. For example in \citet{Sutskever-14} a quality translation of a sentence was made possible with the last output of a recurrent neural network.
%Inspired by the human visual attention mechanism \citep{Fischer-87} that allow us to focus on certain region of an image with high resolution while seeing the rest of the image in low resolution, attention mechanisms \citep{Bahdanau-15} prevent the vanishing long-range dependencies with a weighting mechanism.

In our setting, for a given Tumblr post, the text is broken down into a sequence of words that are embedded into a high-dimensional space (unknown words are mapped to the zero vector) and then fed into an LSTM \citep{Hochreiter-97}. To account for shorter posts, we zero-pad the vector with a special word token. For longer posts, we only keep the 50 first words which is a reasonable choice as 76\% posts in the dataset contain less than 50 words.

\subsection{Evaluation}
The network, named Text Sentiment Model, was trained with the same configuration as the Image Sentiment Model and table \ref{text-results} shows the text model results. Using text alone, the test accuracy is 69\%, almost double the accuracy of the image model evaluated in Section \ref{section:image-evaluation}. This suggests that on Tumblr, text is a better predictor of emotion than images, as we illustrated in Figure \ref{surprised-unclear}.
\begin{table}[H]
\caption{Our text model vs.~random guessing. The words are embedded into vectors of dimension 100 through GloVe, fed to an LSTM of size 1024, and then through a softmax output layer to predict one of 15 emotions.}
\begin{center}
    \begin{tabular}{l | c | c}
    & \textbf{Train} & \textbf{Test} \\
    & \textbf{accuracy} & \textbf{accuracy} \\ \hline
    \textbf{Random guessing} & 11\% & 11\% \\ \hline
    \textbf{Text Sentiment Model}  & 72\% & 69\% \\
    \end{tabular}
\end{center}
\label{text-results}
\end{table}

\section{Deep Sentiment}
Information often comes in several modalities, and humans are able to seamlessly combine them. For instance, in speech recognition, humans integrate audio and visual information to understand speech, as was demonstrated by the McGurk effect \citep{McGurk-76}. Separating what we see from what we hear seems like an easy task, but in an experiment conducted by McGurk, the subjects who were listening to a /{\em ba}/ sound with a visual /{\em ga}/ actually reported they were hearing a /{\em da}/. This is uncanny as even if we know the actual sound is a /{\em ba}/, we cannot stop our brain from interpreting it as a /{\em da}/.

In Figure \ref{surprised-unclear} we gave an example of text being necessary to fully understand the emotion expressed by an image. Sometimes alternative text would lead to entirely different interpretations, as shown in Figure \ref{ambiguous}:

\begin{figure}[H]
    \begin{subfigure}{.48\textwidth}
        \centering
        \includegraphics[width=0.5\linewidth]{Images/scared.jpg}
        \caption{\textbf{Scared}: ``Planes might just be the most frightening thing ever.'' }
    \end{subfigure}~~
    \begin{subfigure}{.48\textwidth}
        \centering
        \includegraphics[width=0.5\linewidth]{Images/scared.jpg}
        \caption{\textbf{Angry}: ``I hate it when people are taking too much space on planes.''}
    \end{subfigure}
    \caption{Different meanings with different captions.}
    \label{ambiguous}
\end{figure}

Exploiting both visual and textual information is therefore key to understanding a user's underlying emotional state. In a survey involving microblog users, \citet{Taochen-17} found that two thirds of the participants added an image to their tweets to enhance the emotion of the text.
We call our proposed network architecture, combining visual recognition and text analysis, ``Deep Sentiment''. As in the previous sections, we evaluate the accuracy of our model below. In Section \ref{section:results} we carefully investigate what psychologically meaningful results we can draw from our model, and whether they match previous results in the psychology literature.

\subsection{Architecture}
Deep Sentiment builds on the models we have seen before as shown in Figure \ref{deep-sentiment}:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/deep-sentiment-structure.pdf}
    \caption{The Deep Sentiment structure. On the one hand, the input image, resized to (224,224,3) is fed into the Inception network and outputs a vector of size 256. On the other hand, the text is projected into a high-dimensional space that subsequently goes through an LSTM layer with 1024 units. The two modalities are then concatenated and fed into a dense layer. The final softmax output layer give the probability distribution over the emotional state of the user.}
    \label{deep-sentiment}
\end{figure}

\subsection{Training}
Deep Sentiment was trained with the same configuration as the previous models and Figure \ref{train-test} shows a comparison of the accuracy curves of the different models:

\begin{figure}[H]
    \begin{subfigure}[t]{.5\textwidth}
        \vskip 0pt %Necessary to align on image and not caption
        \centering
        \includegraphics[width=\linewidth]{Images/train.pdf}
        \caption{Train accuracy}
   \end{subfigure}
   \begin{subfigure}[t]{.5\textwidth}
       \vskip 0pt
       \centering
       \includegraphics[width=\linewidth]{Images/validation.pdf}
       \caption{Test accuracy}
    \end{subfigure}
    \caption{Accuracy comparison of the three models.}
    \label{train-test}
\end{figure}

By combining text and images Deep Sentiment achieves 80\% train accuracy and 72\% test accuracy, significantly outperforming the images-only model and slightly outperforming the text-only model, as shown in Table \ref{all-results} (note that no validation set was used to tune hyperparameters, even though various architectures were tried before yielding Deep Sentiment).

\begin{table}[H]
\caption{Comparison of image model, text model and Deep Sentiment.}
\begin{center}
    \begin{tabular}{ l | c | c | c}
    & \textbf{Loss} & \textbf{Train} & \textbf{Test} \\
    & & \textbf{accuracy} & \textbf{accuracy} \\ \hline
    \textbf{Random guessing} & - & 11\% & 11\% \\ \hline
    \textbf{Image Sentiment Model}  & 1.80 & 43\% & 36\% \\ \hline
    \textbf{Text Sentiment Model} & 0.81 & 72\% & 69\% \\ \hline
    \textbf{Deep Sentiment} & 0.75 & 80\% & 72\% \\
    \end{tabular}
\end{center} 
\label{all-results}
\end{table}

\section{Results}
\label{section:results}
\subsection{Top words for each emotion}
We investigated which words were the most relevant for each emotion as follows: we created artificial posts whose text consisted of a single word -- each of the most frequent 1,000 words in the whole dataset were tested -- accompanied by an image that was the mean image (per channel mean). For each emotion, we report the 10 highest scoring words from these 1,000 artificial posts in Table \ref{top-words} 
(words 11-20 for each emotion are in the Appendix in Section \ref{section:appendix-top-words}).

\begin{table}[H]
\caption{Top 10 words for each emotion, ordered by the relative frequency of the emotion being used as a tag on Tumblr}
\begin{center}
    \begin{tabular}{ l | l}
    Emotion & Top words\\
    \hline
    \textbf{Happy} & healthy, loving, enjoy, wonderful, warm, happiness, smile, lovely, cute, proud\\
    \textbf{Calm} & quiet, situation, peace, mood, towards, warm, slowly, stay, sleep, rain\\
    \textbf{Sad} & horrible, sorry, crying, hurts, tears, cried, lonely, memories, worst, pain\\
    \textbf{Scared} & terrified, scary, panic, nervous, fear, afraid, horrible, woke, happening, worried\\
    \textbf{Bored} & asleep, tired, kinda, busy, stuck, constantly, lonely, sat, listening, depression\\
    \textbf{Angry} & anger, fear, panic, annoying, hate, mad, upset, anxiety, scares, stupid\\
    \textbf{Annoyed} & pissed, ashamed, angry, nervous, speak, surprised, tired, worried, ignore, phone\\
    \textbf{Love} & soul, dreams, happiness, kiss, sex, beauty, women, feelings, god, relationships \\
    \textbf{Excited} & tonight, hopefully, watching, nervous, surprised, expect, tomorrow, amazing, hoping, happen\\
    \textbf{Surprised} & birthday, cried, thank, yesterday, told, sorry, amazing, sweet, friend,  message\\
    \textbf{Optimistic} & positive, expect, surprised, healthy, grow, realize, clearly, hopefully, calm, peace\\
    \textbf{Amazed} & surprised, excited, amazing, woke, realized, awesome, happening, ashamed, yeah, happened\\
    \textbf{Ashamed} & totally, honestly, sorry, absolutely, freaking, honest, completely, stupid, seriously, am\\
    \textbf{Disgusted} & ashamed, totally, angry, hate, stupid, annoyed, horrible, scares, freaking, absolutely\\
    \textbf{Pensive} & mood, wrote, quiet, view, sadness, thoughts, calm, words, sad, kissed\\
    \end{tabular}
\end{center} 
\label{top-words}
\end{table}

An inspection of each of the words suggests that none are out of place, with the possible exception of ``ashamed'' in the list of Amazed words. (This may be due to the fact that the tag Amazed is sometimes used ironically.) Further, our data-driven approach suggests that our methods could be used as an alternative to the word list-based approaches to sentiment analysis common in psychology. The most widely used tool, Linguistic Inquiry Word Count (LIWC), \citep{liwc2007}, consists of dozens of English words which were compiled by hand into psychologically meaningful categories such as ``Health/illness'' or ``Anxiety'' and used in a large number of psychology studies \citep{tausczik2010psychological}. Not only does our approach automatically give sensible word lists, it contains modern words, common in social media usage, like ``woke'' (first attested in its modern meaning in 1962 according to the Oxford English Dictionary which defines it as ``alert to racial or social discrimination and injustice'') and ``phone.'' Woke appears in the top 10 of ``Scared'' and ``Amazed'' and in the top 20 of ``Calm'' (Appendix \ref{section:appendix-top-words}), an interesting finding in its own right. By contrast, both woke and phone appear in LIWC, but not in any of LIWC's emotion word categories, only in a list of verbs and a list of social words, respectively. 

\subsection{Clustering emotions}
Psychologists have long studied the structure of emotion,
and debated whether there are a small number of ``core'' emotions \citep{ekman1992argument},
two dominant factors (see \citet{tellegen1999dimensional} for a discussion of various theories), or more complex models (e.g.~\citet{lindquist2013hundred} critiques previous models and argues that emotions do not belong in natural categories or along multiple dimensions, because they are the ``constructions'' of the human mind, and the result of complex perceptions). 
Classical attempts by psychologists to answer this question have relied on survey data in which respondents can self-report more than one emotion, followed by clustering or factor analysis approaches. More recently, psychologists have turned to neuroscience to address these questions.

While we collected a dataset in which each Tumblr post was labeled with a single emotion tag, we can nevertheless directly address this question using our trained model. Given an image $\mathcal{I}$ and text $\mathcal{T}$ from our dataset, we can compute the conditional probabilities of the emotion classes: $\hat y | \mathcal{I}, \mathcal{T}$ where $\hat y$ is a probability vector in the 15-dimensional simplex. To calculate our model's predicted empirical distribution over $\hat y$ we predict $\hat y$ for all posts in our dataset: $(\hat y_i | \mathcal{I}_i, \mathcal{T}_i)_{i=1}^n$. Finally, we calculate the empirical correlation matrix of $\hat y_1, \ldots, \hat y_n$, which we include in the Appendix in Figure \ref{appendix:heatmap}. For ease of visualization, we convert the correlation matrix to a distance matrix and perform hierarchical clustering as shown in Figure \ref{dendrogram}.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Images/dendrogram_clean.jpg}
    \caption{Hierarchical clustering of the emotion correlation matrix.}
    \label{dendrogram}
\end{figure}

Similar to almost all previous psychology studies, there is a clear distinction between emotions that are \textbf{positive} in nature, such as excited, happy, and love and emotions that are \textbf{negative} such as angry, ashamed, annoyed, and disgusted. Another finding consistent with much previous literature, which we investigate more below, is that within both positive and negative emotions, there is a distinction between  low arousal emotions and high arousal emotions. Bored and sad (low arousal) are in one cluster, while high arousal emotions---angry, ashamed, annoyed, disgusted---are in another. 

\subsection{The circumplex model of emotion}
The circumplex model \citep{posner2005circumplex} posits two dimensions which explain emotions, usually valence (positive vs.~negative) and arousal (low vs.~high), though other dimensions have been suggested. We investigated whether two factors explained most of the variance in our results using principal component analysis (PCA). As shown in the scree plot in the Appendix in Figure \ref{explained-variance}, while the first component explains 28.8\% of the variance, dimensions 2 and 3 explain 16.4\% and 14.6\% respectively, evidence against a simple two factor model.

\begin{figure}[H]
    \begin{subfigure}[t]{.5\textwidth}
        \vskip 0pt %Necessary to align on image and not caption
        \centering
        \includegraphics[width=\linewidth]{Images/PCA_1vs2.png}
        \caption{PCA with a random subset of posts, 150 for each emotion.}
   \end{subfigure}
   \begin{subfigure}[t]{.5\textwidth}
       \vskip 0pt
       \centering
       \includegraphics[width=\linewidth]{Images/variables_pca_1vs2.pdf}
       \caption{PCA on variables}
    \end{subfigure}
    \caption{Affect/valence}
    \label{affect-valence}
\end{figure}
We visualize the results of PCA in Figure \ref{affect-valence}.
At left, a random subset of posts, where for visualization purpose we sampled equally from each emotion class, are shown along the first two principal components, and each is colored by the most probable predicted emotion. This shows that within any particular emotion category there is much variation along these two dimensions, i.e.~they do not cluster very tightly. Related emotions like happy and love are close together, as they were in the dendrogram. 

At right, we visualise the projection of the emotion variables onto the first two principal components. Using this figure, we conclude that the first principal component corresponds well to valence, as it neatly separates happy/love from disgusted/ashamed/annoyed. However, the second principal component is not clearly arousal. In the Appendix, we consider the third principal component as well, but the picture is no clearer. Once again, this provides evidence against a simple two factor model of emotion. Below, we further investigate the valence/arousal model.

\subsection{Validation on the Open Affective Standardized Image Set (OASIS)}
The Open Affective Standardized Image Set (OASIS) dataset \citep{kurdi2017introducing} was developed as an alternative to the  International Affective Picture System (\citep{lang2005international}), a set of emotional stimuli which has been used in many psychology studies. OASIS consists of 900 color images which were rated by human judges on MTurk along the valence and arousal scales discussed above. OASIS images come in classes with labels such as ``Alcohol'', ``Flowers'', or ``Pigeon.'' We applied our Deep Sentiment model to these images, treating the single-word label as the input text. After making predictions, we projected them onto the principal components discussed above. 

In Table \ref{table:oasis} we calculated the correlations between the first three principal components from our model and the mean valence and arousal variables from OASIS. The high correlation between the first principal component (PC1) and the valence variable provides evidence, as in the previous section, that PC1 is capturing a dimension of emotion separating positive from negative. It is also interesting to note the low correlation between PC1 and arousal. By contrast, while PC2 and PC3 are correlated with arousal, the correlations are not very high, and they are also correlated with valence. On the one hand, this is more evidence against there being two factors explaining emotions. On the other hand, asking judges to rate a set of images based on arousal and valence is more similar to the standard sentiment analysis task of predicting whether a sentence expresses positive or negative emotion. By contrast, our dataset comes from a set of images which were chosen by users who also decided to select emotion word tags to accompany them. We consider this to thus be a dataset of richer emotional content, and by design it represents a richer set of emotions.
\begin{table}[H]
\caption{Correlation between OASIS arousal/valence and principal components}
\label{table:oasis}
\begin{center}
    \begin{tabular}{ l | c | c}
    & \textbf{OASIS} & \textbf{OASIS} \\
    & \textbf{valence} & \textbf{arousal} \\ \hline
    \textbf{PC1} & 58\% & 3\% \\ \hline
    \textbf{PC2}  & 17\% & 22\%\\ \hline
    \textbf{PC3} & 30\% & 11\% \\
    \end{tabular}
\end{center} 
\end{table}

\section{Conclusion}
We developed a novel multimodal sentiment analysis method using deep learning methods. Our goal was to investigate a core area of psychology, the study of emotion, using a large and novel social media dataset. Our approach provides new tools for the joint study of images and text on social media. This is important as social science researchers have begun to uncover the important role that images play on social media. As our dataset consisted of text, images, and emotion word tags, we considered it to be a form of ``self-reported'' data, which is the gold standard in emotion. However, it could also be considered to be an unobtrusive behavioral measure, and thus not subject to the biases inherent in laboratory studies. 
But because the data comes from public social media posts, we cannot rule out various sources of bias. What people choose to post---and not to post---on social media is influenced by how they want to present themselves \citep{wojnicki2008word}, so our study is limited insofar as it covers emotion not necessarily as it is truly experienced but rather as it is expressed or performed online. 

In the future, we will evaluate our model on other image / text stimuli datasets that have been developed for psychological studies and investigate whether human judges are more or less accurate than our model. Finally, we will investigate other psychological components of the structure of emotion, for example daily and day of week trends in emotion.

%\subsubsection*{Acknowledgments}

\bibliography{iclr2018_conference}
\bibliographystyle{iclr2018_conference}

\newpage
\appendix

\section{Further results}

\subsection{Top 11-20 words}
\label{section:appendix-top-words}
\begin{table}[H]
\caption{Top 11-20 words for each emotion}
\begin{center}
    \begin{tabular}{ l | l}
    Emotion & Top words\\
    \hline
    \textbf{Happy} & beautiful, beauty, fun, nice, dinner, sweet, good, loves, amazing, comfortable\\
\textbf{Calm} & view, sound, cool, continue, visit, push, woke, morning, safe, step\\
\textbf{Sad} & cry, sick, hurt, dead, feeling, worse, death, upset, falling, panic\\
\textbf{Scared} & kill, happened, cause, asleep, anxiety, worry, hurt, happen, worst, seriously\\
\textbf{Bored} & sitting, suddenly, freaking, slowly, annoying, sometimes, gotten, completely, older, cold\\
\textbf{Angry} & hurt, face, against, cause, hurts, fucking, fight, pain, strong, worst\\
\textbf{Annoyed} & depressed, lately, conversation, constantly, scared, stupid, scares, bored, heard, extremely\\
\textbf{Love} &  sweet, mother, forget, hate, her, song, enjoy, sister, wonderful, dear\\
\textbf{Excited} & 2017, positive, am, definitely, awesome, happens, listening, happy, grow, wanna\\
\textbf{Surprised} & annoyed, negative, apparently, u, minute, asked, sadness, happened, moments, laugh\\
\textbf{Optimistic} & view, yet, believe, tomorrow, trending, said, situation, mood, forward, future\\
\textbf{Amazed} & im, quite, appreciate, honestly, knew, learned, yang, felt, liked, asleep\\
\textbf{Ashamed} & cried, afraid, annoyed, okay, sad, sick, pissed, fucking, depressed, wrong\\
\textbf{Disgusted} & tumblr, pissed, anger, sorry, cried, fucking, terrified, honestly, scared, facebook\\
\textbf{Pensive} &  depression, text, voice, lonely, soul, by, read, truth, sounds, middle\\
    \end{tabular}
\end{center} 
\end{table}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Images/correlation_matrix.pdf}
    \caption{Heatmap of the emotion correlation matrix}
    \label{appendix:heatmap}
\end{figure}
\begin{figure}[H]
    \begin{subfigure}[t]{.5\textwidth}
        \vskip 0pt %Necessary to align on image and not caption
        \centering
        \includegraphics[width=\linewidth]{Images/PCA_1vs3.png}
        \caption{PCA with a random subset of posts, 150 for each emotion}
   \end{subfigure}
   \begin{subfigure}[t]{.5\textwidth}
       \vskip 0pt
       \centering
       \includegraphics[width=\linewidth]{Images/variables_pca_1vs3.pdf}
       \caption{PCA on variables}
    \end{subfigure}
    \caption{PCA on dimension 1 and 3}
\end{figure}

\begin{figure}[H]
    \begin{subfigure}[t]{.5\textwidth}
        \vskip 0pt %Necessary to align on image and not caption
        \centering
        \includegraphics[width=\linewidth]{Images/PCA_2vs3.png}
        \caption{PCA with a random subset of posts, 150 for each emotion}
   \end{subfigure}
   \begin{subfigure}[t]{.5\textwidth}
       \vskip 0pt
       \centering
       \includegraphics[width=\linewidth]{Images/variables_pca_2vs3.pdf}
       \caption{PCA on variables}
    \end{subfigure}
    \caption{PCA for dimension 2 and 3}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Images/explained_variance.pdf}
    \caption{Explained variance of the PCA}
    \label{explained-variance}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Images/factor_loadings.pdf}
    \caption{Factor loadings}
\end{figure}


\end{document}

