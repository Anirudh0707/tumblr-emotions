{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from reader import ptb_raw_data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "happy = pd.read_csv('data/happy.csv', encoding='utf-8')\n",
    "text = happy['text'][pd.notnull(happy['text'])]\n",
    "# Add extra space at the end of each post\n",
    "# so that after concatenation they remain separated\n",
    "chars = text.map(lambda s: list(s) + [u' '])\n",
    "# Flatten chars\n",
    "chars = [item for sublist in chars for item in sublist]\n",
    "# Replace space by the special character underscore\n",
    "chars = [u'_' if x == ' ' else x for x in chars]\n",
    "\n",
    "# Map char to id\n",
    "df_chars = pd.DataFrame({'char': chars})\n",
    "char_count = df_chars.groupby('char')['char'].count()\n",
    "reduced_chars = char_count[char_count > 10000]\n",
    "char_to_id = dict(zip(reduced_chars.index, np.arange(reduced_chars.shape[0], dtype=int)))\n",
    "unknown_char_id = len(char_to_id)\n",
    "\n",
    "train_data = np.array(map(lambda c: char_to_id.get(c, unknown_char_id), chars))[:5000000]\n",
    "char_to_id[u'<unk>'] = len(char_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#np.savetxt('data/happy_chars.txt', train_data, fmt='%d')\n",
    "#np.save('data/char_to_id.npy', char_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'data/happy_chars.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a03b2af18121>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Only contains 5M chars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfull_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/happy_chars.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mchar_to_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/char_to_id.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mid_to_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchar_to_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python2.7/site-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin)\u001b[0m\n\u001b[1;32m    856\u001b[0m                 \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbz2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBZ2File\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m                 \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'U'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m                 \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'data/happy_chars.txt'"
     ]
    }
   ],
   "source": [
    "# Only contains 5M chars\n",
    "full_data = np.loadtxt('data/happy_chars.txt', dtype=int)[:500000]\n",
    "char_to_id = np.load('data/char_to_id.npy').item()\n",
    "id_to_char = {i: k for k, i in char_to_id.iteritems()}\n",
    "\n",
    "# Split into train/val/test 80/10/10\n",
    "train_split = (int)(full_data.shape[0] * 0.8)\n",
    "val_split = (int)(full_data.shape[0] * 0.9)\n",
    "train_data = full_data[:train_split]\n",
    "valid_data = full_data[train_split:val_split]\n",
    "test_data = full_data[val_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data, char_to_id = ptb_raw_data('data_chars', char=True)\n",
    "id_to_char = {i: k for k, i in char_to_id.iteritems()}\n",
    "config = {'char_to_id': char_to_id,\n",
    "          'id_to_char': id_to_char,\n",
    "          'batch_size': 20,\n",
    "          'num_steps': 20,\n",
    "          'vocab_size': len(char_to_id),\n",
    "          'hidden_size': 200,\n",
    "          'num_layers': 2, # Number of stacked LSTMs\n",
    "          'dropout': 0.9, # Proba to keep neurons\n",
    "          'max_grad_norm': 5.0, # Maximum norm of gradient\n",
    "          'init_scale': 0.1, # Weights initialization scale\n",
    "          'initial_lr': 1.0,\n",
    "          'lr_decay': 0.5,\n",
    "          'max_epoch_no_decay': 4, # Number of epochs without decaying learning rate\n",
    "          'nb_epochs': 10} # Maximum number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class CharModel():\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        batch_size = config['batch_size']\n",
    "        num_steps = config['num_steps']\n",
    "        vocab_size = config['vocab_size']\n",
    "        hidden_size = config['hidden_size']\n",
    "        num_layers = config['num_layers']\n",
    "        dropout = config['dropout']\n",
    "        max_grad_norm = config['max_grad_norm']\n",
    "        initial_lr = config['initial_lr']\n",
    "        \n",
    "        self.input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        self.target = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        self.learning_rate = tf.Variable(initial_lr, trainable=False)\n",
    "        # Use a placeholder to turn off dropout during testing \n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        # Char embedding\n",
    "        #embedding = tf.get_variable('embedding', [vocab_size, hidden_size])\n",
    "        #input_embed = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "        #input_embed_dropout = tf.nn.dropout(input_embed, self.keep_prob)\n",
    "        input_data_one_hot = tf.one_hot(self.input_data, vocab_size)\n",
    "\n",
    "        # LSTM\n",
    "        def lstm_cell():\n",
    "            return tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, forget_bias=1.0)\n",
    "        def lstm_cell_dropout():\n",
    "            return tf.contrib.rnn.DropoutWrapper(lstm_cell(), output_keep_prob=self.keep_prob)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([lstm_cell_dropout() for _ in range(num_layers)], state_is_tuple=True)\n",
    "        self.initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        state = self.initial_state\n",
    "        outputs = []\n",
    "        with tf.variable_scope('RNN'):\n",
    "            for t in range(num_steps):\n",
    "                if t > 0: tf.get_variable_scope().reuse_variables() # Reuse the weights in the LSTMs\n",
    "                output, state = cell(input_data_one_hot[:, t, :], state)\n",
    "                outputs.append(output)\n",
    "        self.final_state = state\n",
    "\n",
    "        h1 = tf.reshape(tf.stack(outputs, axis=1), [-1, hidden_size])\n",
    "        W_softmax = tf.get_variable('W_softmax', [hidden_size, vocab_size])\n",
    "        b_softmax = tf.get_variable('b_softmax', [vocab_size])\n",
    "        logits = tf.matmul(h1, W_softmax) + b_softmax\n",
    "        logits = tf.reshape(logits, [batch_size, num_steps, vocab_size])\n",
    "        # Use sequence loss for average over batch and sum across timesteps\n",
    "        loss_vector = tf.contrib.seq2seq.sequence_loss(logits, self.target, weights=tf.ones([batch_size, num_steps]),\n",
    "                                                       average_across_batch=True, average_across_timesteps=False)\n",
    "        self.loss = tf.reduce_sum(loss_vector)\n",
    "        # Use gradient cliping\n",
    "        trainable_vars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, trainable_vars), max_grad_norm)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "        self.train_step = optimizer.apply_gradients(zip(grads, trainable_vars),\n",
    "                                                    global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "        self.predict = tf.cast(tf.argmax(tf.reshape(logits, [-1, vocab_size]), 1), tf.int32)\n",
    "        correct_pred = tf.equal(self.predict, tf.reshape(self.target, [-1]))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def run_model(sess, model, data, is_training, show_loss_graph=False):\n",
    "    batch_size = model.config['batch_size']\n",
    "    num_steps = model.config['num_steps']\n",
    "    dropout = model.config['dropout']\n",
    "    initial_lr = model.config['initial_lr']\n",
    "    lr_decay = model.config['lr_decay']\n",
    "    max_epoch_no_decay = model.config['max_epoch_no_decay']\n",
    "    nb_epochs = model.config['nb_epochs']\n",
    "    \n",
    "    batch_len = data.shape[0] / batch_size\n",
    "    data = data[:batch_len * batch_size].reshape((batch_size, batch_len))\n",
    "    epoch_size = (batch_len - 1) / num_steps\n",
    "    if is_training:\n",
    "        # Iteration to print at\n",
    "        print_iter = list(np.linspace(0, epoch_size - 1, 11).astype(int))\n",
    "        dropout_param = dropout\n",
    "        ops = [model.final_state, model.loss, model.accuracy, model.train_step]\n",
    "    else:\n",
    "        dropout_param = 1.0\n",
    "        ops = [model.final_state, model.loss, model.accuracy, tf.no_op()]\n",
    "\n",
    "    for e in range(nb_epochs):\n",
    "        print ('Epoch: {0}'.format(e + 1))\n",
    "        lr_decay = lr_decay ** max(e + 1 - max_epoch_no_decay, 0)\n",
    "        sess.run(tf.assign(model.learning_rate, initial_lr * lr_decay))\n",
    "\n",
    "        total_loss = 0.0\n",
    "        total_accuracy = 0.0\n",
    "        nb_iter = 0.0\n",
    "        perplexity_history = []\n",
    "        numpy_state = sess.run(model.initial_state)\n",
    "        t0 = time()\n",
    "        for i in range(epoch_size):\n",
    "            curr_input = data[:, i * num_steps: (i + 1) * num_steps]\n",
    "            # Target is the input shifted in time by 1\n",
    "            curr_target = data[:, i * num_steps + 1: (i + 1) * num_steps + 1]\n",
    "            numpy_state, curr_loss, curr_acc, _ = sess.run(ops,\n",
    "                                                           feed_dict={model.input_data: curr_input, \n",
    "                                                                      model.target: curr_target,\n",
    "                                                                      model.initial_state: numpy_state, \n",
    "                                                                      model.keep_prob: dropout_param})\n",
    "            total_loss += curr_loss\n",
    "            total_accuracy += curr_acc\n",
    "            nb_iter += num_steps\n",
    "            perplexity_history.append(np.exp(curr_loss / num_steps))\n",
    "\n",
    "            if (is_training and i in print_iter):\n",
    "                print('{0:.0f}% perplexity = {1:.3f}, accuracy = {2:.3f}, speed = {3:.0f} cps'\\\n",
    "                      .format(print_iter.index(i) * 10, \n",
    "                              np.exp(total_loss / nb_iter), total_accuracy / (i + 1),\n",
    "                              (nb_iter * batch_size) / (time() - t0)))\n",
    "        if not is_training:\n",
    "            print('Perplexity = {0:.3f}, accuracy = {1:.3f}, speed = {2:.0f} cps'\\\n",
    "                  .format(np.exp(total_loss / nb_iter), total_accuracy / (i + 1),\n",
    "                          (nb_iter * batch_size) / (time() - t0)))\n",
    "\n",
    "        if (is_training and show_loss_graph):\n",
    "            plt.plot(perplexity_history)\n",
    "            plt.grid(True)\n",
    "            plt.title('Epoch {0}'.format(e + 1))\n",
    "            plt.xlabel('Mini-batch number')\n",
    "            plt.ylabel('Perplexity per mini-batch')\n",
    "            plt.show()\n",
    "            \n",
    "def generate_chars(sess, model, first_char, max_iteration):\n",
    "    ops = [model.final_state, model.predict]\n",
    "    current_char = first_char.copy()\n",
    "    numpy_state = sess.run(model.initial_state)\n",
    "    preds = []\n",
    "    for i in range(max_iteration):\n",
    "        numpy_state, pred = sess.run(ops, feed_dict={model.input_data: current_char,\n",
    "                                                     model.initial_state: numpy_state,\n",
    "                                                     model.keep_prob: 1.0})\n",
    "        preds.append(pred[0])\n",
    "        current_chars = pred.reshape((1, 1))\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "Epoch: 1\n",
      "0% perplexity = 49.962, accuracy = 0.072, speed = 1182 cps\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    print('Training:')\n",
    "    init_scale = config['init_scale']\n",
    "    initializer = tf.random_uniform_initializer(-init_scale, init_scale)    \n",
    "    with tf.variable_scope('Model', reuse=None, initializer=initializer):\n",
    "        config['nb_epochs'] = 1\n",
    "        m_train = CharModel(config)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    run_model(sess, m_train, train_data, is_training=True)\n",
    "    print('\\nValidation:')\n",
    "    with tf.variable_scope('Model', reuse=True):\n",
    "        config['nb_epochs'] = 1\n",
    "        m_valid = CharModel(config)\n",
    "    #run_model(sess, m_valid, valid_data, is_training=False)\n",
    "    print('\\nTest:')\n",
    "    with tf.variable_scope('Model', reuse=True):\n",
    "        m_test =  CharModel(config)\n",
    "    #run_model(sess, m_test, test_data, is_training=False)\n",
    "    print('\\nCharacters generation')\n",
    "    with tf.variable_scope('Model', reuse=True):\n",
    "        config['batch_size'] = 1\n",
    "        config['num_steps'] = 1\n",
    "        m_gen = CharModel(config)\n",
    "    first_char = np.array([[4]])\n",
    "    preds = generate_chars(sess, m_gen, first_char, 100)\n",
    "    generated_chars = map(lambda x: config['id_to_char'][x], preds)\n",
    "    np.save('generated_chars.npy', np.array(generated_chars))\n",
    "    print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'                                                                                                    '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_chars = np.load('generated_chars.npy')\n",
    "''.join(list(gene_chars)).replace('_', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
