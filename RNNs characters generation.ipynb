{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from reader import ptb_raw_data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "happy = pd.read_csv('data/happy.csv', encoding='utf-8')\n",
    "text = happy['text'][pd.notnull(happy['text'])]\n",
    "# Add extra space at the end of each post\n",
    "# so that after concatenation they remain separated\n",
    "chars = text.map(lambda s: list(s) + [u' '])\n",
    "# Flatten chars\n",
    "chars = [item for sublist in chars for item in sublist]\n",
    "# Replace space by the special character underscore\n",
    "chars = [u'_' if x == ' ' else x for x in chars]\n",
    "\n",
    "# Map char to id\n",
    "df_chars = pd.DataFrame({'char': chars})\n",
    "char_count = df_chars.groupby('char')['char'].count()\n",
    "reduced_chars = char_count[char_count > 10000]\n",
    "char_to_id = dict(zip(reduced_chars.index, np.arange(reduced_chars.shape[0], dtype=int)))\n",
    "unknown_char_id = len(char_to_id)\n",
    "\n",
    "train_data = np.array(map(lambda c: char_to_id.get(c, unknown_char_id), chars))[:5000000]\n",
    "char_to_id[u'<unk>'] = len(char_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#np.savetxt('data/happy_chars.txt', train_data, fmt='%d')\n",
    "#np.save('data/char_to_id.npy', char_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Only contains 5M chars\n",
    "full_data = np.loadtxt('data/happy_chars.txt', dtype=int)[:500000]\n",
    "char_to_id = np.load('data/char_to_id.npy').item()\n",
    "id_to_char = {i: k for k, i in char_to_id.iteritems()}\n",
    "\n",
    "# Split into train/val/test 80/10/10\n",
    "train_split = (int)(full_data.shape[0] * 0.8)\n",
    "val_split = (int)(full_data.shape[0] * 0.9)\n",
    "train_data = full_data[:train_split]\n",
    "val_data = full_data[train_split:val_split]\n",
    "test_data = full_data[val_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#train_data, valid_data, test_data, word_to_id = ptb_raw_data('data', char=True)\n",
    "#id_to_word = {i: k for k, i in word_to_id.iteritems()}\n",
    "#vocab_size = len(word_to_id)\n",
    "vocab_size = len(char_to_id)\n",
    "num_steps = 20\n",
    "hidden_size = 200\n",
    "batch_size = 20\n",
    "num_layers = 2 # Number of stacked LSTMs\n",
    "dropout = 0.9 # Proba to keep neurons\n",
    "max_grad_norm = 5.0 # Maximum norm of gradient\n",
    "init_scale = 0.1 # Weights initialization scale\n",
    "initial_lr = 1.0\n",
    "lr_decay = 0.5\n",
    "max_epoch_no_decay = 4 # Number of epochs not decaying learning rate\n",
    "nb_epochs = 10 # Maximum number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "input_data = tf.placeholder(tf.int32, [None, num_steps])\n",
    "target = tf.placeholder(tf.int32, [None, num_steps])\n",
    "learning_rate = tf.Variable(initial_lr, trainable=False)\n",
    "# Use a placeholder to turn off dropout during testing \n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "def build_graph():\n",
    "    # Char embedding\n",
    "    #embedding = tf.get_variable('embedding', [vocab_size, hidden_size])\n",
    "    #input_embed = tf.nn.embedding_lookup(embedding, input_data)\n",
    "    #input_embed_dropout = tf.nn.dropout(input_embed, keep_prob)\n",
    "    input_data_one_hot = tf.one_hot(input_data, vocab_size)\n",
    "\n",
    "    # LSTM\n",
    "    def lstm_cell():\n",
    "        return tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, forget_bias=1.0)\n",
    "    def lstm_cell_dropout():\n",
    "        return tf.contrib.rnn.DropoutWrapper(lstm_cell(), output_keep_prob=keep_prob)\n",
    "    hidden_cell = lstm_cell_dropout\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([hidden_cell() for _ in range(num_layers)], state_is_tuple=True)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    state = initial_state\n",
    "    outputs = []\n",
    "    with tf.variable_scope('RNN'):\n",
    "        for t in range(num_steps):\n",
    "            if t > 0: tf.get_variable_scope().reuse_variables() # Reuse the weights in the LSTMs\n",
    "            output, state = cell(input_data_one_hot[:, t, :], state)\n",
    "            outputs.append(output)\n",
    "    final_state = state\n",
    "\n",
    "    h1 = tf.reshape(tf.stack(outputs, axis=1), [-1, hidden_size])\n",
    "    W_softmax = tf.get_variable('W_softmax', [hidden_size, vocab_size])\n",
    "    b_softmax = tf.get_variable('b_softmax', [vocab_size])\n",
    "    logits = tf.matmul(h1, W_softmax) + b_softmax\n",
    "    logits = tf.reshape(logits, [batch_size, num_steps, vocab_size])\n",
    "    # Use sequence loss for average over batch and sum across timesteps\n",
    "    loss_vector = tf.contrib.seq2seq.sequence_loss(logits, target, weights=tf.ones([batch_size, num_steps]),\n",
    "                                                   average_across_batch=True, average_across_timesteps=False)\n",
    "    loss = tf.reduce_sum(loss_vector)\n",
    "    # Use gradient cliping\n",
    "    trainable_vars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, trainable_vars), max_grad_norm)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_step = optimizer.apply_gradients(zip(grads, trainable_vars),\n",
    "                                           global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "    predict = tf.cast(tf.argmax(tf.reshape(logits, [-1, vocab_size]), 1), tf.int32)\n",
    "    correct_pred = tf.equal(predict, tf.reshape(target, [-1]))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    return initial_state, final_state, loss, accuracy, train_step, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def run_model(sess, data, is_training, lr_decay=0.5, nb_epochs=1, show_loss_graph=False):\n",
    "    batch_len = data.shape[0] / batch_size\n",
    "    data = data[:batch_len * batch_size].reshape((batch_size, batch_len))\n",
    "    epoch_size = (batch_len - 1) / num_steps\n",
    "    if is_training:\n",
    "        # Iteration to print at\n",
    "        print_iter = list(np.linspace(0, epoch_size - 1, 11).astype(int))\n",
    "        initializer = tf.random_uniform_initializer(-init_scale, init_scale)    \n",
    "        with tf.variable_scope('Model', reuse=None, initializer=initializer):\n",
    "            initial_state, final_state, loss, accuracy, train_step, _ = build_graph()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # Dropout parameter\n",
    "        dropout_param = dropout\n",
    "        ops = [final_state, loss, accuracy, train_step]\n",
    "    else:\n",
    "        with tf.variable_scope('Model', reuse=True):\n",
    "            initial_state, final_state, loss, accuracy, _, _ = build_graph()\n",
    "        dropout_param = 1.0\n",
    "        ops = [final_state, loss, accuracy, tf.no_op()]\n",
    "\n",
    "    for e in range(nb_epochs):\n",
    "        print ('Epoch: {0}'.format(e + 1))\n",
    "        lr_decay = lr_decay ** max(e + 1 - max_epoch_no_decay, 0)\n",
    "        sess.run(tf.assign(learning_rate, initial_lr * lr_decay))\n",
    "\n",
    "        total_loss = 0.0\n",
    "        total_accuracy = 0.0\n",
    "        nb_iter = 0.0\n",
    "        perplexity_history = []\n",
    "        numpy_state = sess.run(initial_state)\n",
    "        t0 = time()\n",
    "        for i in range(epoch_size):\n",
    "            curr_input = data[:, i * num_steps: (i + 1) * num_steps]\n",
    "            # Target is the input shifted in time by 1\n",
    "            curr_target = data[:, i * num_steps + 1: (i + 1) * num_steps + 1]\n",
    "            numpy_state, curr_loss, curr_acc, _ = sess.run(ops,\n",
    "                                                           feed_dict={input_data: curr_input, \n",
    "                                                                      target: curr_target,\n",
    "                                                                      initial_state: numpy_state, \n",
    "                                                                      keep_prob: dropout_param})\n",
    "            total_loss += curr_loss\n",
    "            total_accuracy += curr_acc\n",
    "            nb_iter += num_steps\n",
    "            perplexity_history.append(np.exp(curr_loss / num_steps))\n",
    "\n",
    "            if (is_training and i in print_iter):\n",
    "                print('{0:.0f}% perplexity = {1:.3f}, accuracy = {2:.3f}, speed = {3:.0f} cps'\\\n",
    "                      .format(print_iter.index(i) * 10, \n",
    "                              np.exp(total_loss / nb_iter), total_accuracy / (i + 1),\n",
    "                              (nb_iter * batch_size) / (time() - t0)))\n",
    "        if not is_training:\n",
    "            print('Perplexity = {0:.3f}, accuracy = {1:.3f}, speed = {2:.0f} cps'\\\n",
    "                  .format(np.exp(total_loss / nb_iter), total_accuracy / (i + 1),\n",
    "                          (nb_iter * batch_size) / (time() - t0)))\n",
    "\n",
    "        if (is_training and show_loss_graph):\n",
    "            plt.plot(perplexity_history)\n",
    "            plt.grid(True)\n",
    "            plt.title('Epoch {0}'.format(e + 1))\n",
    "            plt.xlabel('Mini-batch number')\n",
    "            plt.ylabel('Perplexity per mini-batch')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_chars(sess, first_chars, max_iteration):\n",
    "    with tf.variable_scope('Model', reuse=True):\n",
    "        initial_state, final_state, _, _, _, predict = build_graph()\n",
    "        ops = [final_state, predict]\n",
    "\n",
    "    # Can only handle batches of size batch_size, fill with zeros, need to change that in the future\n",
    "    current_chars = np.concatenate([first_chars, \n",
    "                                    np.zeros((batch_size - 1, num_steps), dtype=np.int32)])\n",
    "    numpy_state = sess.run(initial_state)\n",
    "    preds = []\n",
    "    for i in range(max_iteration):\n",
    "        numpy_state, pred = sess.run(ops, feed_dict={input_data: current_chars,\n",
    "                                                     initial_state: numpy_state,\n",
    "                                                     keep_prob: 1.0})\n",
    "        preds.append(pred[:num_steps])\n",
    "        current_chars = np.concatenate([pred[:num_steps].reshape((1, -1)), \n",
    "                                        np.zeros((batch_size - 1, num_steps), dtype=np.int32)])\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "Epoch: 1\n",
      "0% perplexity = 95.132, accuracy = 0.032, speed = 1194 cps\n",
      "10% perplexity = 36.526, accuracy = 0.125, speed = 3937 cps\n",
      "20% perplexity = 33.341, accuracy = 0.129, speed = 3964 cps\n",
      "30% perplexity = 32.546, accuracy = 0.128, speed = 4042 cps\n",
      "40% perplexity = 31.809, accuracy = 0.128, speed = 4134 cps\n",
      "50% perplexity = 31.662, accuracy = 0.127, speed = 4185 cps\n",
      "60% perplexity = 31.505, accuracy = 0.126, speed = 4231 cps\n",
      "70% perplexity = 31.550, accuracy = 0.125, speed = 4264 cps\n",
      "80% perplexity = 31.441, accuracy = 0.125, speed = 4275 cps\n",
      "90% perplexity = 31.357, accuracy = 0.125, speed = 4297 cps\n",
      "100% perplexity = 31.163, accuracy = 0.125, speed = 4300 cps\n",
      "\n",
      "Validation:\n",
      "Epoch: 1\n",
      "Perplexity = 36.284, accuracy = 0.073, speed = 13288 cps\n",
      "\n",
      "Test:\n",
      "Epoch: 1\n",
      "Perplexity = 33.028, accuracy = 0.079, speed = 14290 cps\n",
      "\n",
      "Characters generation\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print('Training:')\n",
    "    run_model(sess, train_data, lr_decay=lr_decay, is_training=True, nb_epochs=1)\n",
    "    print('\\nValidation:')\n",
    "    run_model(sess, val_data, is_training=False, nb_epochs=1)\n",
    "    print('\\nTest:')\n",
    "    run_model(sess, test_data, is_training=False, nb_epochs=1)\n",
    "    print('\\nCharacters generation')\n",
    "    first_chars = train_data[:num_steps].reshape((1, -1))\n",
    "    preds = generate_chars(sess, first_chars, 50)\n",
    "    generated_chars = map(lambda x: id_to_char[x], np.stack(preds, axis=0).reshape((-1)))\n",
    "    np.save('generated_chars.npy', np.array(generated_chars))\n",
    "    print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'errrtt t n sappy t ooneanlu  tn tou teaw thet tappyng   an ah sou tlappy #lntaarar #hove  ile henet   eaae#o#ettlpi totn    d na heth rth r ahe  th ey eet  ndt eto raoen e tsoaegti i#sape ##n  ta    t  nll iulnaooon#o  e#   e ettar eteaeteane a hetn an  t ee tn rtend  anch  l   nansalar#sha##h ##  h  eoewn        aua nsa##  r erea   r   r  anlar etndt h n theteart t ndo t ot  etd  i eaaanpaa#pc##a#     et          io  la##e#   ra  e   e   d   a  ee tatethe   r  th e rthmn t n et n ntlliirliiotanc#####  t             ovkiap###et          o        to    e a  et hetnceaeeeah etn tete eenn  nn    allaac##o###           e nn ialno#####                           aa   o l     tn eth  ran  et eew    l ntiaancaa#####        oetn    lllac#####                             o        n et etdt n an  t   o e nnl o llaaap######     et   ooe elallc#####                                     an eetet dt h     nseaiwntoo  liillccc#####        ano ieotllsc#####                                      r aoi at    e  rn   rra ovn  ooomlssc#####      an r eo omeacc#####                                     n          eat  eancneet n   e  omaacc#####   te nnan  riomaaap#####                                              o    o    aet  n  n n iomaaap###h #eet t enn   liilaac######                                                     a o  e etn  n liilaa#pnr #h#n  t  ovn   illlacc######                                                        et e ikn   lioa#sapa##h## e t  n oe eoallccc#####                                                      ncn et  ovn iiliriiapp# #h##e   tnr  oooomescc#####                                                    tn a  e tnn nenn  it<unk>sappn###heew n      oomaacp#####                                                       oeete  et n    ieeticapr##e###      rliiacacc######                                                     ra n aet   n   ne ietaaccc##### einn e aomaaapp#####                                                   e       o   e tn  rlioomaaap# n i#n#nr n liitiacc#####                                                            oet einn   llii<unk>e nsaaaa#e#ikn nn omlaap######                                                         anneet  ookn   et ll inlnaa#ae#n en iillccc######                                                         a    et  n toown   tlllrla#nd#nn oaaomesccc#####                                                             o h   et  he eeenleaaea#n t    ooomcccc#####                                                          e       a pnr  e  l  laah        oooomaccp#####                                                               ooea    o  ov                aoeecaap######                                                             a       et                   onliiaaa<unk><unk>######                                                                                       nn ll <unk><unk><unk><unk><unk><unk><unk>######                                                                                   toow<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>######                                                                                  <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>######                                                                              <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>######                                                                          <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>######                                                                      <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>######                                                                  <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>######                                                              <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>######                                                          <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>######                                                      <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>######                                                  <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>######                                              <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>######                                          <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>######                                      <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>######                                  <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>######                              <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>######                          <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>######                      <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>######                  <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>######              <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>######          <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>######      <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_chars = np.load('generated_chars.npy')\n",
    "''.join(list(gene_chars)).replace('_', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Training:\n",
    "Epoch: 1\n",
    "0% perplexity = 49.808, accuracy = 0.007, speed = 1063 wps\n",
    "10% perplexity = 23.576, accuracy = 0.133, speed = 3847 wps\n",
    "20% perplexity = 23.342, accuracy = 0.134, speed = 3882 wps\n",
    "30% perplexity = 22.138, accuracy = 0.146, speed = 3887 wps\n",
    "40% perplexity = 16.137, accuracy = 0.222, speed = 3893 wps\n",
    "50% perplexity = 12.636, accuracy = 0.283, speed = 3898 wps\n",
    "60% perplexity = 10.569, accuracy = 0.329, speed = 3901 wps\n",
    "70% perplexity = 9.244, accuracy = 0.362, speed = 3903 wps\n",
    "80% perplexity = 8.348, accuracy = 0.388, speed = 3906 wps\n",
    "90% perplexity = 7.686, accuracy = 0.409, speed = 3872 wps\n",
    "100% perplexity = 7.182, accuracy = 0.426, speed = 3875 wps\n",
    "\n",
    "\n",
    "Validation:\n",
    "Epoch: 1\n",
    "Perplexity = 3.897, accuracy = 0.583, speed = 12705 wps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "one hot \n",
    "Training:\n",
    "Epoch: 1\n",
    "0% perplexity = 50.221, accuracy = 0.018, speed = 1232 cps\n",
    "10% perplexity = 23.491, accuracy = 0.135, speed = 3908 cps\n",
    "20% perplexity = 23.316, accuracy = 0.135, speed = 4344 cps\n",
    "30% perplexity = 23.293, accuracy = 0.135, speed = 4495 cps\n",
    "40% perplexity = 23.332, accuracy = 0.135, speed = 4518 cps\n",
    "50% perplexity = 19.760, accuracy = 0.173, speed = 4437 cps\n",
    "60% perplexity = 15.712, accuracy = 0.230, speed = 4394 cps\n",
    "70% perplexity = 13.037, accuracy = 0.277, speed = 4333 cps\n",
    "80% perplexity = 11.250, accuracy = 0.314, speed = 4261 cps\n",
    "90% perplexity = 9.965, accuracy = 0.345, speed = 4290 cps\n",
    "100% perplexity = 9.014, accuracy = 0.371, speed = 4314 cps\n",
    "\n",
    "\n",
    "Validation:\n",
    "Epoch: 1\n",
    "Perplexity = 3.660, accuracy = 0.604, speed = 13735 cps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
