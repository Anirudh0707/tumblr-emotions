{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from text_model.text_embedding import compute_sklearn_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#for i in range(30):\n",
    "    #index = np.random.randint(df_all.shape[0])\n",
    "    #print(df_all['search_query'][index])\n",
    "    #print(df_all['text'][index])\n",
    "    #print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading word embedding weights.\n",
      "Finished loading dataframes.\n",
      "Computing sklearn features:\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "X_sklearn, y_sklearn = compute_sklearn_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(295424, 50)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sklearn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "_RANDOM_SEED = 0\n",
    "X_sklearn_train, X_sklearn_valid, y_sklearn_train, y_sklearn_valid = train_test_split(X_sklearn, y_sklearn, \n",
    "                                                                                      test_size=0.2,\n",
    "                                                                                      random_state=_RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.519279509518\n",
      "0.517864094102\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_sklearn_train, y_sklearn_train)\n",
    "print(accuracy_score(logreg.predict(X_sklearn_train), y_sklearn_train))\n",
    "print(accuracy_score(logreg.predict(X_sklearn_valid), y_sklearn_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.582438784966\n",
      "0.511043412034\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(max_depth=12)\n",
    "forest.fit(X_sklearn_train, y_sklearn_train)\n",
    "print(accuracy_score(forest.predict(X_sklearn_train), y_sklearn_train))\n",
    "print(accuracy_score(forest.predict(X_sklearn_valid), y_sklearn_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading word embedding weights.\n",
      "Finished loading dataframes.\n",
      "Training:\n"
     ]
    }
   ],
   "source": [
    "from text_model.text_embedding import main\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Epoch: 10\n",
    "0%  loss = 1.150, accuracy = 0.555, speed = 430 pps\n",
    "10%  loss = 1.061, accuracy = 0.593, speed = 5735 pps\n",
    "20%  loss = 1.063, accuracy = 0.590, speed = 5972 pps\n",
    "30%  loss = 1.065, accuracy = 0.589, speed = 6057 pps\n",
    "40%  loss = 1.064, accuracy = 0.589, speed = 6093 pps\n",
    "50%  loss = 1.064, accuracy = 0.589, speed = 6116 pps\n",
    "60%  loss = 1.063, accuracy = 0.589, speed = 6122 pps\n",
    "70%  loss = 1.063, accuracy = 0.589, speed = 6135 pps\n",
    "80%  loss = 1.062, accuracy = 0.589, speed = 6145 pps\n",
    "90%  loss = 1.063, accuracy = 0.589, speed = 6145 pps\n",
    "100%  loss = 1.063, accuracy = 0.589, speed = 6150 pps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading word embedding weights.\n"
     ]
    }
   ],
   "source": [
    "from datasets.download_images import download_im_with_text\n",
    "\n",
    "search_query = 'surprised'\n",
    "start = 0\n",
    "end = 100\n",
    "download_im_with_text(search_query, start, end, dataset_dir='data', subdir='photos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading word embedding weights.\n",
      ">> Converting image 215/215 shard 4\n",
      ">> Converting image 50/50 shard 4\n",
      "\n",
      "Finished converting the dataset!\n"
     ]
    }
   ],
   "source": [
    "from datasets.convert_images_tfrecords import convert_images_with_text\n",
    "\n",
    "convert_images_with_text('data', num_valid=50, photos_subdir='photos', tfrecords_subdir='tfrecords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From image_model/im_model.py:207: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead.\n",
      "WARNING:tensorflow:From //anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:394: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From //anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "WARNING:tensorflow:From image_model/im_model.py:208: get_total_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.get_total_loss instead.\n",
      "WARNING:tensorflow:From //anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:261: get_losses (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.get_losses instead.\n",
      "WARNING:tensorflow:From //anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:263: get_regularization_losses (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.get_regularization_losses instead.\n",
      "INFO:tensorflow:Starting Session.\n",
      "INFO:tensorflow:Starting Queues.\n",
      "INFO:tensorflow:global_step/sec: 0\n",
      "INFO:tensorflow:global step 1: loss = 2.5468 (24.51 sec/step)\n",
      "Step 0: loss: 2.547, validation accuracy: 0.100\n",
      "INFO:tensorflow:global step 2: loss = 2.3731 (14.32 sec/step)\n",
      "Step 1: loss: 2.373, validation accuracy: 0.100\n",
      "INFO:tensorflow:global step 3: loss = 2.4964 (13.87 sec/step)\n",
      "Step 2: loss: 2.496, validation accuracy: 0.100\n",
      "INFO:tensorflow:global step 4: loss = 2.4450 (13.65 sec/step)\n",
      "Step 3: loss: 2.445, validation accuracy: 0.100\n",
      "INFO:tensorflow:global step 5: loss = 2.5533 (13.69 sec/step)\n",
      "Step 4: loss: 2.553, validation accuracy: 0.100\n",
      "INFO:tensorflow:Stopping Training.\n",
      "INFO:tensorflow:Finished training! Saving model to disk.\n",
      "Finished training. Last batch loss 2.553\n"
     ]
    }
   ],
   "source": [
    "from image_model.im_model import fine_tune_model_with_text\n",
    "\n",
    "dataset_dir = 'data'\n",
    "checkpoints_dir = 'image_model/pretrained_model'\n",
    "train_dir = 'image_model/fine_tuned_model'\n",
    "num_steps = 5\n",
    "learning_rate = 1e-7\n",
    "fine_tune_model_with_text(dataset_dir, checkpoints_dir, train_dir,\n",
    "                          num_steps, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From //anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/training/python/training/evaluation.py:298: __init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n",
      "INFO:tensorflow:Waiting for new checkpoint at image_model/fine_tuned_model\n",
      "INFO:tensorflow:Found new checkpoint at image_model/fine_tuned_model/model.ckpt-5\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-30-18:26:06\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-30-18:26:14\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-a012610d58b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m       \u001b[0mnum_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_evals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0meval_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames_to_updates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m       eval_interval_secs=600)\n\u001b[0m",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/evaluation.pyc\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(master, checkpoint_dir, logdir, num_evals, initial_op, initial_op_feed_dict, eval_op, eval_op_feed_dict, final_op, final_op_feed_dict, summary_op, summary_op_feed_dict, variables_to_restore, eval_interval_secs, max_number_of_evaluations, session_config, timeout)\u001b[0m\n\u001b[1;32m    283\u001b[0m       \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m       \u001b[0mmax_number_of_evaluations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_number_of_evaluations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m       timeout=timeout)\n\u001b[0m",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/training/python/training/evaluation.pyc\u001b[0m in \u001b[0;36mevaluate_repeatedly\u001b[0;34m(checkpoint_dir, master, scaffold, eval_ops, feed_dict, final_ops, final_ops_feed_dict, eval_interval_secs, hooks, config, max_number_of_evaluations, timeout)\u001b[0m\n\u001b[1;32m    521\u001b[0m   \u001b[0mnum_evaluations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m   for checkpoint_path in checkpoints_iterator(checkpoint_dir,\n\u001b[0;32m--> 523\u001b[0;31m                                               eval_interval_secs, timeout):\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     session_creator = monitored_session.ChiefSessionCreator(\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/training/python/training/evaluation.pyc\u001b[0m in \u001b[0;36mcheckpoints_iterator\u001b[0;34m(checkpoint_dir, min_interval_secs, timeout)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0mtime_to_next_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmin_interval_secs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtime_to_next_eval\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_to_next_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import slim\n",
    "from image_model import inception_v1\n",
    "from datasets import dataset_utils\n",
    "from datasets.convert_to_dataset import get_split, get_split_with_text\n",
    "from image_model.im_model import _load_batch\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "    dataset_dir = 'data'\n",
    "    # Load validation data\n",
    "    image_size = inception_v1.default_image_size\n",
    "    dataset_valid = get_split_with_text('validation', dataset_dir)\n",
    "    images_valid, _, labels_valid = _load_batch(dataset_valid, batch_size=dataset_valid.num_samples, shuffle=False, \n",
    "                                                height=image_size, width=image_size)\n",
    "        \n",
    "    # Create the model, use the default arg scope to configure the batch norm parameters.\n",
    "    with slim.arg_scope(inception_v1.inception_v1_arg_scope()):\n",
    "        logits_valid, _ = inception_v1.inception_v1(images_valid, num_classes=dataset_valid.num_classes, \n",
    "                                                    is_training=False, reuse=True)\n",
    "    # Accuracy metrics\n",
    "    accuracy_valid = slim.metrics.streaming_accuracy(tf.cast(labels_valid, tf.int32),\n",
    "                                           tf.cast(tf.argmax(logits_valid, 1), tf.int32))\n",
    "    #mse_valid = slim.metrics.accuracy(tf.cast(labels_valid, tf.int32),\n",
    "     #                                           tf.cast(tf.argmax(logits_valid, 1), tf.int32))\n",
    "\n",
    "    # Choose the metrics to compute:\n",
    "    names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\n",
    "        'http://localhost:8888/notebooks/Documents/tumblr-sentiment/Word%20embedding.ipynb#accuracy': accuracy_valid,\n",
    "    })\n",
    "        \n",
    "    checkpoint_dir = 'image_model/fine_tuned_model'\n",
    "    log_dir = '/tmp/my_model_eval/'\n",
    "    # We'll evaluate 1000 batches:\n",
    "    num_evals = 1\n",
    "    # Evaluate every 10 minutes:\n",
    "    slim.evaluation.evaluation_loop(\n",
    "      '',\n",
    "      checkpoint_dir,\n",
    "      log_dir,\n",
    "      num_evals=num_evals,\n",
    "      eval_op=names_to_updates.values(),\n",
    "      eval_interval_secs=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/39732460/how-to-use-evaluation-loop-with-train-loop-in-tf-slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
