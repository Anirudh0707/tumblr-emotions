\chapter{Natural Language Processing}
Even as a human being, it can be difficult to guess the expressed emotion by only looking at a Tumblr image without reading its caption as shown by Figure \ref{disgusted-unclear}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Images/disgusted.jpg}
    \caption{Which emotion is it?}
    \label{disgusted-unclear}
\end{figure}

It's unclear whether the user wants to convey sadness or disgust. Only by reading the text ``Me when I see a couple expressing their affection in physical ways in public", we can finally conclude that the person was {\em disgusted}. The text is extremely informative and is usually crucial to accurately infer the expressed emotion.

Neural networks only accept numbers as inputs, therefore the text has to be converted beforehand. A very successful way to capture the meaning of textual information is by using word embedding.
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%  NEW SECTION   %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Word Embedding}
One way to transform text into numbers would be to use a dictionary that maps each word in a given vocabulary (containing all the words of every Tumblr posts) to an unique integer. Then, we could transform any word into an one-hot vector -- a vector of the same size as the vocabulary, with a 1 in the position of the word and 0s elsewhere. A sentence could then be encoded as a sum of vectors, that can be normalised by some distance ($L^2$ for instance).

A major drawback of that representation is data sparsity as the vocabulary size can be huge. For example, the number of 5-word sentences with a vocabulary size of 1000, is $1000^5=10^{15}$. This sparsity problem is specific to text as in contrary, image and audio processing systems train on rich high-dimensional data (pixel intensities for images and spectral densities for audio), as shown by Figure \ref{comparison-text}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Images/comparison-text.png}
    \caption{Data sparsity in text \cite{comparison-text}}
    \label{comparison-text}
\end{figure}

Most learning algorithms rely on the local smoothness hypothesis, that is, similar training instances are spatially close. This hypothesis clearly doesn't hold with one-hot encoding as `dog' is as close to `tree' as it is to `cat'. Ideally, we would like to transform the word `dog' in a space so that it's closer to `cat' than it is to `tree'. That's exactly how \textbf{word embedding} works: every word is projected into a high-dimensional space that preserves semantic relationships. Therefore, what the model has learned about dogs can be used when faced with a cat.

\subsection{Word2Vec Overview}
The Word2Vec model by T. Mikolov et al. \cite{word2vec} is an efficient implementation of word embedding that learned the high-dimensional representation of words with a {\em fake task}. The idea is to train a neural network with a single hidden layer on a task, but we call the task {\em fake} as the network will never be used: we're only after the weights of the hidden layer the model will learn.

Now to explain the fake task, suppose we have a sentence: ``the ants in the garden". We can break that sentence in (context, target) pairs where the contexts are the words surrounding the target word. For example, if we take a context with a window of size 1, we get the following pairs: ([the, in], ants), ([ants, the], in), ([in, garden], the). We will then train a model to predict the target word given the context, and the weights of the model will give the word embedding. This model is called the Continuous Bag-of-Words model, and Word2Vec also comes in another flavor called the Skip-Gram model.

In the Skip-Gram model, we will instead predict the context given the target word, by creating more pairs as for instance ([the, in], ants) are split into two training instances: (ants, the) and (ants, in). The Continuous Bag-of-Words model smooth over the distributional information by using the whole context, and works well on smaller datasets, but by breaking (context, target) pairs into more observations, the Skip-Gram model tends to perform better on larger datasets, and that's the model we will stick on from now on.

\subsection{Skip-Gram Model}
The Skip-Gram model is a neural network with one hidden layer and its objective is to predict the context word given the target. The input of the network will be a target word represented as a one-hot vector, of size say 10,000 (that's the vocabulary size) and the output will also be a vector of size 10,000 giving the probability distribution of the context word. Figure \ref{fig:skip-gram} shows the architecture of the model:

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Images/word2vec-architecture.png}
    \caption{Skip-Gram model architecture \cite{word2vec-architecture}, with a hidden layer of size 300}
    \label{fig:skip-gram}
\end{figure}

There is no activation function in the hidden layer, but output neurons go through softmax to obtain a probability distribution of the context word. The weights of the hidden layer matrix give the embedding as when we multiply a $1\times$10,000 one-hot vector with a 10,000$\times300$ matrix (the hidden layer weights) we select the row of size $1\times300$ corresponding to the high-dimensional representation of that word:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/onehot-matrix.png}
    \caption{Word embedding origin\cite{word2vec-architecture}}
\end{figure}

\newpage
To learn those weights, the network is minimising the cross-entropy loss given a pair (target word, context word) = $(w_t, w_c)$:
\begin{align}
    \text{J}_{\text{CE}}(w_t, w_c)   &=  -\text{log}~P(w_c | w_t) \nonumber\\
    &= -\text{log}~\{\text{softmax}(w_c, w_t)\}\nonumber\\
    &= -\text{log}\left(\frac{\text{exp}\{\text{score}(w_c, w_t)\}}{\sum_{\text{Word w' in Vocab}}\text{exp}\{\text{score}(w', w_t)\}} \right)
    \label{cross-entropy}
\end{align}

With $\text{score}(w_c, w_t)$ being the element in position $w_c$ in the output layer (right before the softmax activation function).

\subsection{Intuition}
In this model, two words that have similar contexts should output similar probability distributions. One way to produce that is to simply learn a similar word embedding for these two words. Therefore, words with similar context will have a similar vector representation which is exactly what we wanted.

Which words have similar contexts? Synonyms are a good example: `brave' and `fearless' are two words that must appear in similar contexts. The same applies for words that are related such as `physics' and `thermodynamics' or words with the same stem, e.g. `apples' and 'apple'. 

\newpage
\section{Word2Vec Training}
Training the network described above involves, with a vocabulary size of 10,000: 10,000$\times300$=3,000,000 parameters for the hidden layer and $300\times$10,000=3,000,000 for the output layer. And in fact, the actual Word2Vec vocabulary contain 3M words: the number of parameters is thus 2$\times$3,000,000$\times$300=1,800,000,000. That's a huge neural network that will need a really large training set to train those parameters. Word2Vec uses a few tricks to obtain word representations of high quality at a reduced computational cost:

\begin{enumerate}
    \item \textbf{Negative sampling}, to dramatically reduce the training time complexity.
    \item \textbf{Word pairs and phrases}, to add frequently occurring groups of words in the vocabulary.
    \item \textbf{Subsampling of frequent words}, to downsize the effects of words occurring too often.
\end{enumerate}

\subsection{Negative Sampling}
When training the model with gradient descent, each backward pass will update all the parameters of the model. Negative sampling addresses this problem by only updating a fraction of the parameters.

For a given (target word, context word) pair, we want the output of the model to be 1 on the context word and 0s for all the other words. With negative sampling, we'll instead randomly select a small subset of `negative samples' (words we want the network to output a 0) to update the weights for. The paper by T. Mikolov et al. \cite{word2vec2} states that 5-20 negative samples for small datasets and 2-5 for large datasets achieve good results.

More specifically, the negative examples are sampled using a `unigram distribution' with more frequent words more likely to be selected. The probability to select a word $w_i$ is simply its frequency $f_i$ to the power 3/4 (chosen empirically) divided by the sum of the frequencies of the $V$ other words ($V$ been the vocabulary size):
\begin{equation}
    P(w_i) = \frac{f_i^{3/4}}{\sum_{j=1}^V f_j^{3/4}}
\end{equation}

If we select 5 negative samples, then in the output layer those 5 words and the context word will be updated. As they each have 300 parameters in the output layer (the embedding size), only 6$\times$300=1,800 parameters will be updated among the 0.9B parameters in the output layer: or in other words, 0.0002\% of the parameters, which considerably speeds up the training.

In the hidden layer, only the weights of the input word (300 parameters) will be updated, but that's always the case regardless of negative sampling as the one-hot vector representation of the input word zero-out every weight in the hidden layer that does not belong to the input word.

\subsection{Negative Sampling, with Maths}
The loss function (\ref{cross-entropy}) has a normalising denominator that is expensive to compute, and is the direct cause of why all the parameters in the output layer would be updated. We'd like to instead use an approximation with a loss cheaper to compute.

Instead of discriminating the context word $w_c$ from all the other words in the vocabulary, we'll sample $k$ words $\tilde{w}_1, \tilde{w}_2, ..., \tilde{w}_k$ from the unigram distribution $Q$, called the noise distribution, that we'll discriminate from the context word $w_c$, as shown in Figure \ref{discri}.

\begin{figure}[H]
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{Images/discri-softmax.png}
        \caption{Softmax loss}
    \end{subfigure}
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{Images/discri-noise.png}
        \caption{Negative Sampling loss}
    \end{subfigure}
    \caption{Comparison of the two loss functions}
    \label{discri}
\end{figure}

The new binary classification task has $w_c$ as positive example $(y=1)$ and all the noise samples $\tilde{w}_1, \tilde{w}_2, ..., \tilde{w}_k$  as negative examples ($y=0$). The loss function, with a pair $(w_t, w_c)$, is:
\begin{equation}
    \text{J}_{\text{NEG}}(w_t, w_c)  = -[\text{log}~P(y=1| w_c, w_t) + k\mathbb{E}_{\tilde{w}\sim Q}[\text{log}~P(y=0|\tilde{w}, w_t)]]
\end{equation}

Calculating the expectation $\mathbb{E}_{\tilde{w}\sim Q}$ would involve summing over all the vocabulary to compute the probability normalising constant, which is exactly what we wanted to avoid in the first place. That's why we will instead use a Monte Carlo approximation with our noise samples:
\begin{align}
    \text{J}_{\text{NEG}}(w_t, w_c)  &= -\left[\text{log}~P(y=1| w_c, w_t) + k\sum_{j=1}^{k}\frac{1}{k}\text{log}~P(y=0|\tilde{w}_j, w_t)\right]\nonumber \\
     &= -\left[\text{log}~P(y=1| w_c, w_t) + \sum_{j=1}^k\text{log}~P(y=0|\tilde{w}_j, w_t)\right]
\end{align}

We still haven't given a proper derivation of the probability $P(y=1| w_c, w_t)$: note that for each context word $w_c$ given its target word $w_t$, we're generating $k$ noise samples from a distribution $Q$. There are two distributions at stake: the distribution $P_{\text{train}}$ of the context word $w_c$ given $w_t$ which is simply the softmax computed earlier:
\begin{equation}
    P_{\text{train}}(w_c|w_t) = \text{softmax}(w_c, w_t)
\end{equation}
And the unigram distribution $Q$ to sample the noise:
\begin{equation}
    Q(w_i) = \frac{f_i^{3/4}}{\sum_{j=1}^V f_j^{3/4}}
\end{equation}

The probability to obtain a positive example is simply a weighted probability of seeing an example from $P_{\text{train}}$ \cite{ruder}:
\begin{align}
    P(y=1| w_c, w_t) &= \frac{\frac{1}{k+1}P_{\text{train}}(w_c|w_t)}{\frac{1}{k+1}P_{\text{train}}(w_c|w_t) + \frac{k}{k+1}Q(w_c)}\nonumber \\
     &= \frac{P_{\text{train}}(w_c|w_t)}{P_{\text{train}}(w_c|w_t) + kQ(w_c)}
\end{align}

Therefore, as $P(y=0| w_c, w_t) = 1 - P(y=1| w_c, w_t)$:
\begin{equation}
    P(y=0| w_c, w_t) = \frac{kQ(w_c)}{P_{\text{train}}(w_c|w_t) + kQ(w_c)}
\end{equation}

Computing $P_{\text{train}}(w_c|w_t)$ remains expensive as it involves the softmax normalising factor $Z(w_t)$:
\begin{align}
    P_{\text{train}}(w_c|w_t) &= \frac{\text{exp}\{\text{score}(w_c, w_t)\}}{\sum_{\text{Word w' in Vocab}}\text{exp}\{\text{score}(w', w_t)\}} \nonumber \\
     &= \frac{\text{exp}\{\text{score}(w_c, w_t)\}}{Z(w_t)}
\end{align}

The trick to avoid computing $Z(w_t)$ is to simply consider it as another parameter the model has to learn. Mnih and Teh \cite{nce} actually set the parameter to 1 as they report that it doesn't affect the performance. This statement was bolstered by B. Zoph et al. \cite{nce2} who found that this parameter was close to 1 with a low variance. Setting $Z(w_t)$ to 1 gives:
\begin{equation}
    P(y=1| w_c, w_t) = \frac{\text{exp}\{\text{score}(w_c, w_t)\}}{\text{exp}\{\text{score}(w_c, w_t)\} + kQ(w_c)}
\end{equation}

The loss function becomes:
\begin{equation}
    \text{J}_{\text{NEG}}(w_t, w_c) = -\left[\text{log}~\frac{\text{exp}\{\text{score}(w_c, w_t)\}}{\text{exp}\{\text{score}(w_c, w_t)\} + kQ(w_c)} + \sum_{j=1}^k\text{log}~\frac{kQ(\tilde{w}_j)}{\text{exp}\{\text{score}(\tilde{w}_j, w_t)\} + kQ(\tilde{w}_j)}\right]
\end{equation}

Actually, this loss function is not quite Negative Sampling, but instead what we call Noise Contrastive Estimation (NCE). Negative Sampling has a further simplification we'll discuss shortly. Mnih and Teh \cite{nce} proved that as the number of noise samples $k$ increases, the gradient of the NCE goes toward the gradient of the softmax function. In their paper, they also state that 25 samples are enough to match the performance of the softmax, and with an increased speed of a factor 45.

\newpage
The remaining expensive term to compute in the loss function is $kQ(w_c)$, as it involves computing the unigram distribution over all the vocabulary. This isn't as expensive as calculating the normalising factor $Z(w_t)$ since the noise distribution only needs to be computed once and can be then stored in a vector during the whole training. However, in Negative Sampling, this most expensive term $kQ(w_c)$ is set to 1 \cite{word2vec2}. This is actually true when $k=V$ and $Q$ is an uniform distribution.	 Now $P(y=1| w_c, w_t)$ is actually a sigmoid function:
\begin{equation}
    P(y=1| w_c, w_t) = \frac{1}{1 + \text{exp}\{-\text{score}(w_c, w_t)\}}
\end{equation}

Giving the following final loss function:
\begin{equation}
    \text{J}_{\text{NEG}}(w_t, w_c) = -\left[\text{log}~\frac{1}{1 + \text{exp}\{-\text{score}(w_c, w_t)\}} + \sum_{j=1}^k\text{log}~\frac{1}{1 + \text{exp}\{\text{score}(\tilde{w}_j, w_t)\}}\right]
\end{equation}

\subsection{Word Pairs and Phrases}
`Times Higher Education' has a different meaning than `times', `higher' and `education' taken separately, it's therefore sensible to add that kind of phrases in the vocabulary. Ideally, we would like to also not add word pairs such as `that is' or `and are' to the vocabulary as they make more sense being separated. We want to group words that are frequent together but infrequent in general. To do so, we use the following scoring function of two words $w_i$ and $w_j$:
\begin{equation}
    \text{score}(w_i, w_j) = \frac{\text{count}(w_iw_j) - \delta}{\text{count}(w_i)\times \text{count}(w_j)}\times |W|
\end{equation}
With: 
\begin{itemize}[topsep=0pt]
    \item $\text{count}(w_i)$ the number of time the word $w_i$ appear in the corpus (all the training data).
    \item $\text{count}(w_iw_j)$ the number of consecutive occurrence of both $w_i$ and $w_j$ in the corpus. 
    \item $\delta$ a discounting coefficient to prevent creating too many phrases made up of very infrequent words.
    \item $|W|$ the training set size, in order to make the threshold more independent of the training size.
\end{itemize}

The pairs $(w_i, w_j)$ with a score above a threshold are then added to the vocabulary. Several passes on the training data are made to make longer phrases such as `Times Higher Education' (usually there are 2-4 passes with a decreasing threshold value after each pass).

\subsection{Subsampling of Frequent Words}
If we look again at the example ``the ants in the garden", the training instances will contain (ants, the) and (garden, the) [note that in Skip-Gram, (target, context) pairs can also be made of words that have incomplete context, that is no left or no right context word such as the target word `garden' that only have a left context word `the']. The word `the' doesn't help a lot at understanding the context of the words `ant' and `garden' as it appears in virtually every noun. Furthermore, `the' will have far more training instances than are actually needed to get a quality vector representation of that word.

To address these problems, subsampling is used: each word in the corpus has a probability to be deleted relative to its frequency. Therefore, if we have a window size of 10 and the word `the' is deleted, then this word will not appear in the context of the remaining words. Also, we now have 10 fewer training instances containing `the'.

The probability of keeping a word $w_i$ with frequency $f_i$ is:
\begin{equation}
    P(w_i) = \frac{t}{f_i} + \sqrt{\frac{t}{f_i}}
    \label{subsampling}
\end{equation}
With $t$ a parameter controlling how aggressive subsampling is, smaller value of $t$ means more subsampling. 

\newpage
Figure \ref{subsampling} shows how the probability to keep a word decreases with word frequency:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/subsampling_prob.eps}
    \caption{Subsampling probability}
    \label{subsampling}
\end{figure}

Note that the formula (\ref{subsampling}) is different from the one in \cite{word2vec2}, but we decided to keep (\ref{subsampling}) as it was used in the actual C implementation of Word2Vec. Note that if $t=1\mathrm{e}{-3}$:
\begin{itemize}
    \item $P(w_i) = 1.0$ for $f_i\leq0.0026$, meaning that subsampling will only affect words that represent more than 0.26\% of the words.
    \item $P(w_i) = 0.5$ for $f_i=0.0075$, any word representing 0.75\% of the words will have a fifty-fifty chance of being dropped.
\end{itemize}

\subsection{Word2Vec Pre-Trained Model}

For our task of emotion prediction, we will be using a pre-trained model to convert the text in Tumblr posts into rich high-dimensional vectors. The Word2Vec model was trained on Google News dataset (aggregation of news from all around the world, including blog posts which should be similar to Tumblr content) containing about 100 billion words. Each word in the vocabulary (3 million words) are embedded into a 300 dimensional vector. 

The Skip-Gram model was trained using the following parameters:
\begin{itemize}
    \item A context window of size 10.
    \item 5 negative examples in Negative Sampling.
    \item In Word Pairs and Phrases, the discounting factor $\delta$ is set to 100, and the threshold to 100. The number of passes could not be found unfortunately.
    \item A subsampling threshold of $1\mathrm{e}{-3}.$
\end{itemize}

\section{Application to Tumblr Data}
Each post in the dataset does not necessarily contain the same number of words. Even after embedding each word, the input will be of variable size and most learning algorithm expect a fixed-sized input. To solve that problem, we can simply average across the number of words. The information loss is still minimal as the features come from a high-dimensional space \cite{seth1}.

However note that the word order is completely lost. Human language relies on the word order to communicate as for example the word {\em change} can be both a noun and a verb, and negation such as `not entertained' can only be understood if `not' directly precedes the verb.

The order information can be preserved using Recurrent Neural Networks, our next chapter.







