\appendix
\setcounter{chapter}{0}
% \numberline{} aligns 'Appendix' with the rest of the chapters
\addcontentsline{toc}{chapter}{\numberline{}Appendix}
\chapter{Python Code}
\lstset{language=Python}
\lstset{frame=lines}
\lstset{basicstyle=\scriptsize}
\lstset{showstringspaces=false}

\section{Tumblr Data}

\subsection{Data Extraction}
\begin{lstlisting}
import numpy as np

def extract_tumblr_posts(client, nb_requests, search_query, before, delta_limit):
    """Extract Tumblr posts with a given emotion.
    
    Parameters:
        client: Authenticated Tumblr client with the pytumblr package.
        nb_requests: Number of API request.
        search_query: Emotion to search for.
        before: A timestamp to search for posts before that value.
        delta_limit: Maximum difference of timestamp between two queries.
    """    
    for i in range(nb_requests):
        tagged = client.tagged(search_query, filter='text', before=before)
        nb_rejected = 0
        timestamps_rejected = []
        for elt in tagged:
            timestamp = elt['timestamp']
            if (abs(timestamp - before) < delta_limit):
                before = timestamp

                current_post = []
                current_post.append(elt['id'])
                current_post.append(elt['post_url'])

                elt_type = elt['type']
                current_post.append(elt_type)
                current_post.append(timestamp)
                current_post.append(elt['date'])
                current_post.append(elt['tags'])
                current_post.append(elt['liked'])
                current_post.append(elt['note_count'])

                if (elt_type == 'photo'):
                    # Only take the first image
                    current_post.append(elt['photos'][0]['original_size']['url'])
                    current_post.append(elt['caption'].replace('\n',' ').replace('\r',' '))
                    current_post.append(search_query)
                    posts.append(current_post)
                elif (elt_type == 'text'):
                    current_post.append(np.nan)
                    current_post.append(elt['body'].replace('\n',' ').replace('\r',' '))
                    current_post.append(search_query)
                    posts.append(current_post)
            else:
                nb_rejected += 1
                timestamps_rejected.append(timestamp)

            return (posts, nb_rejected, timestamps_rejected)
\end{lstlisting}

\subsection{Data Preprocessing and Conversion for TensorFlow}
\begin{lstlisting}
import os
import urllib2
import io
import random
import sys
import math

import pandas as pd
import tensorflow as tf

from PIL import Image
from scipy.misc import imread, imresize
from slim.nets import inception
from datasets import dataset_utils
from text_model.text_preprocessing import preprocess_one_df
from text_model.text_preprocessing import _load_embedding_weights_glove

def download_im_with_text(search_query, start, end, dataset_dir='data', subdir='photos'):
    """Download images using the urls in the dataframe specified by the search query.

    Parameters:
        search_query: A string giving the sentiment to load the corresponding dataframe.
        start: A start index for the loaded dataframe.
        end: An end index for the loaded dataframe.
        dataset_dir: A directory where the dataframes are stored.
        subdir: A subdirectory to store the photos.

    Returns:
        Images downloaded in the directory dataset_dir/subdir/search_query, having 
        the posts ids as names.
    """
    # Load data
    emb_name = 'glove'
    text_dir = 'text_model'
    emb_dir = 'embedding_weights'
    filename = 'glove.6B.50d.txt'
    if emb_name == 'word2vec':
        vocabulary, embedding = _load_embedding_weights_word2vec(text_dir, emb_dir, filename)
    else:
        vocabulary, embedding = _load_embedding_weights_glove(text_dir, emb_dir, filename)

    df = preprocess_one_df(vocabulary, embedding, search_query, _POST_SIZE)
    links = df['photo']
    # Create subdir if it doesn't exist
    if not tf.gfile.Exists(os.path.join(dataset_dir, subdir)):
        tf.gfile.MakeDirs(os.path.join(dataset_dir, subdir))
    # Create search_query folder if it doesn't exist
    photos_dir = os.path.join(dataset_dir, subdir, search_query)
    if not tf.gfile.Exists(photos_dir):
        tf.gfile.MakeDirs(photos_dir)
    for i in range(start, end):
        # Check for NaNs
        if links[i] == links[i]:
            # Open url and convert to JPEG image
            try:
                f = urllib2.urlopen(links[i])
            except Exception:
                continue
            image_file = io.BytesIO(f.read())
            im = Image.open(image_file)
            # The filename is the index of the image in the dataframe
            filename = str(i) + '.jpg'
            im.convert('RGB').save(os.path.join(photos_dir, filename), 'JPEG')
            
%convert to tfrecords
def convert_images_with_text(dataset_dir, num_valid, photos_subdir='photos', 
    tfrecords_subdir='tfrecords'):
    """Downloads the photos and convert them to TFRecords.

    Parameters:
        dataset_dir: The data directory.
        photos_subdir: The subdirectory where the photos are stored.
        tfrecords_subdir: The subdirectory to store the TFRecords files.
    """
    # Create the tfrecords_subdir if it doesn't exist
    if not tf.gfile.Exists(os.path.join(dataset_dir, tfrecords_subdir)):
        tf.gfile.MakeDirs(os.path.join(dataset_dir, tfrecords_subdir))

    if _dataset_exists(dataset_dir, photos_subdir):
        print('Dataset files already exist. Exiting without re-creating them.')
        return

    photo_filenames, class_names = _get_filenames_and_classes(dataset_dir, photos_subdir)
    class_names_to_ids = dict(zip(class_names, range(len(class_names))))

    # Divide into train and test:
    random.seed(_RANDOM_SEED)
    random.shuffle(photo_filenames)
    training_filenames = photo_filenames[num_valid:]
    validation_filenames = photo_filenames[:num_valid]

    # Load dataframes
    df_dict = dict()
    emotions = ['happy', 'sad', 'scared', 'angry', 'surprised', 'disgusted']
    emb_name = 'glove'
    text_dir = 'text_model'
    emb_dir = 'embedding_weights'
    filename = 'glove.6B.50d.txt'
    if emb_name == 'word2vec':
        vocabulary, embedding = _load_embedding_weights_word2vec(text_dir, emb_dir, filename)
    else:
        vocabulary, embedding = _load_embedding_weights_glove(text_dir, emb_dir, filename)

    for emotion in emotions:
        df_dict[emotion] = preprocess_one_df(vocabulary, embedding, emotion, _POST_SIZE)

    # First, convert the training and validation sets.
    _convert_dataset_with_text('train', training_filenames, class_names_to_ids,
                               dataset_dir, df_dict, tfrecords_subdir)
    _convert_dataset_with_text('validation', validation_filenames, class_names_to_ids,
                               dataset_dir, df_dict, tfrecords_subdir)

    # Write the train/validation split size
    train_valid_split = dict(zip(['train', 'validation'], [len(photo_filenames) - num_valid, 
        num_valid]))
    train_valid_filename = os.path.join(dataset_dir, photos_subdir, _TRAIN_VALID_FILENAME)
    with tf.gfile.Open(train_valid_filename, 'w') as f:
        for split_name in train_valid_split:
            size = train_valid_split[split_name]
            f.write('%s:%d\n' % (split_name, size))

    # Finally, write the labels file:
    labels_to_class_names = dict(zip(range(len(class_names)), class_names))
    dataset_utils.write_label_file(labels_to_class_names, dataset_dir, photos_subdir)

    #_clean_up_temporary_files(dataset_dir)
    print('\nFinished converting the dataset!')
    
%get_split
def get_split_with_text(split_name, dataset_dir, photos_subdir='photos', 
  tfrecords_subdir='tfrecords', file_pattern=None, reader=None):
    """Gets a dataset tuple with instructions for reading tumblr data.

    Args:
        split_name: A train/validation split name.
        dataset_dir: The base directory of the dataset sources.
        photos_subdir: The subdirectory containing the photos.
        tfrecords_subdir: The subdirectory containing the TFRecords files.
        file_pattern: The file pattern to use when matching the dataset sources.
            It is assumed that the pattern contains a '%s' string so that the split
            name can be inserted.
        reader: The TensorFlow reader type.

    Returns:
        A `Dataset` namedtuple.

    Raises:
        ValueError: if `split_name` is not a valid train/validation split.
    """
    #if split_name not in SPLITS_TO_SIZES:
        #raise ValueError('split name %s was not recognized.' % split_name)

    if not file_pattern:
        file_pattern = _FILE_PATTERN
    file_pattern = os.path.join(dataset_dir, tfrecords_subdir, file_pattern % split_name)

    # Allowing None in the signature so that dataset_factory can use the default.
    if reader is None:
        reader = tf.TFRecordReader

    keys_to_features = {
      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),
      'image/format': tf.FixedLenFeature((), tf.string, default_value='jpg'),
      'image/class/label': tf.FixedLenFeature(
          [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),
      'text': tf.FixedLenFeature(
          [_POST_SIZE], tf.int64, default_value=tf.zeros([_POST_SIZE], dtype=tf.int64)),
    }

    items_to_handlers = {
      'image': slim.tfexample_decoder.Image(),
      'text': slim.tfexample_decoder.Tensor('text'),
      'label': slim.tfexample_decoder.Tensor('image/class/label'),
    }

    decoder = slim.tfexample_decoder.TFExampleDecoder(
        keys_to_features, items_to_handlers)

    labels_to_names = None
    if dataset_utils.has_labels(dataset_dir, photos_subdir):
        labels_to_names = dataset_utils.read_label_file(dataset_dir, photos_subdir)

    # Get split size
    train_valid_filename = os.path.join(dataset_dir, photos_subdir, _TRAIN_VALID_FILENAME)
    with tf.gfile.Open(train_valid_filename, 'rb') as f:
        lines = f.read().decode()
    lines = lines.split('\n')
    lines = filter(None, lines)

    train_valid_split = {}
    for line in lines:
        index = line.index(':')
        train_valid_split[line[:index]] = (int)(line[index+1:])

    return slim.dataset.Dataset(
      data_sources=file_pattern,
      reader=reader,
      decoder=decoder,
      num_samples=train_valid_split[split_name],
      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,
      num_classes=len(labels_to_names),
      labels_to_names=labels_to_names)
\end{lstlisting}

\newpage
\section{Visual Recognition}
\begin{lstlisting}
""" Fine-tune a pre-trained Inception model by chopping off the last logits layer. 
"""
import os
import sys

import numpy as np
import tensorflow as tf

from tensorflow.contrib import slim
from tensorflow.contrib.slim.python.slim.learning import train_step
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

from slim.preprocessing import inception_preprocessing
#from slim.nets import inception
from image_model import inception_v1
from datasets import dataset_utils
from datasets.convert_to_dataset import get_split, get_split_with_text
from datasets.convert_images_tfrecords import get_numpy_data

# Seed for reproducibility
_RANDOM_SEED = 0

def download_pretrained_model(url, checkpoint_dir):
    """Download pretrained inception model and store it in checkpoint_dir.

    Parameters:
        url: The url containing the compressed model.
        checkpoint_dir: The directory to save the model.
    """
    if not tf.gfile.Exists(checkpoint_dir):
        tf.gfile.MakeDirs(checkpoint_dir)
    dataset_utils.download_and_uncompress_tarball(url, checkpoint_dir)

def _load_batch(dataset, batch_size=32, shuffle=True, height=299, width=299, 
    is_training=False):
    """Load a single batch of data. 
    
    Args:
      dataset: The dataset to load.
      batch_size: The number of images in the batch.
      shuffle: Whether to shuffle the data sources and common queue when reading.
      height: The size of each image after preprocessing.
      width: The size of each image after preprocessing.
      is_training: Whether or not we're currently training or evaluating.
    
    Returns:
      images: A Tensor of size [batch_size, height, width, 3], image samples that have 
          been preprocessed.
      images_raw: A Tensor of size [batch_size, height, width, 3], image samples that 
                           can be used for visualization.
      labels: A Tensor of size [batch_size], whose values range between 0 and dataset.num_classes.
    """
    # For validation, if you set the common_queue_capacity to something lower than
    # batch_size, which is the validation size, then your output will contain duplicates.
    data_provider = slim.dataset_data_provider.DatasetDataProvider(
        dataset, shuffle=shuffle, common_queue_capacity=batch_size,
        common_queue_min=8)
    image_raw, label = data_provider.get(['image', 'label'])
    
    # Preprocess image for usage by Inception.
    image = inception_preprocessing.preprocess_image(image_raw, height, width, 
        is_training=is_training)

    # Preprocess the image for display purposes.
    image_raw = tf.expand_dims(image_raw, 0)
    image_raw = tf.image.resize_images(image_raw, [height, width])
    image_raw = tf.squeeze(image_raw)

    # Batch it up.
    images, images_raw, labels = tf.train.batch(
        [image, image_raw, label],
        batch_size=batch_size,
        num_threads=1,
        capacity=2 * batch_size)
    
    return images, images_raw, labels

def load_batch_with_text(dataset, batch_size=32, shuffle=True, height=299, width=299, 
    is_training=False):
    """Load a single batch of data. 
    
    Args:
      dataset: The dataset to load.
      batch_size: The number of images in the batch.
      shuffle: Whether to shuffle the data sources and common queue when reading.
      height: The size of each image after preprocessing.
      width: The size of each image after preprocessing.
      is_training: Whether or not we're currently training or evaluating.
    
    Returns:
      images: A Tensor of size [batch_size, height, width, 3], image samples that have been 
          preprocessed.
      images_raw: A Tensor of size [batch_size, height, width, 3], image samples that can 
          be used for visualization.
      labels: A Tensor of size [batch_size], whose values range between 0 and dataset.num_classes.
    """
    # For validation, if you set the common_queue_capacity to something lower than
    # batch_size, which is the validation size, then your output will contain duplicates.
    data_provider = slim.dataset_data_provider.DatasetDataProvider(
        dataset, shuffle=shuffle, common_queue_capacity=batch_size,
        common_queue_min=8)
    image_raw, text, label = data_provider.get(['image', 'text', 'label'])
    
    # Preprocess image for usage by Inception.
    image = inception_preprocessing.preprocess_image(image_raw, height, width, 
        is_training=is_training)

    # Preprocess the image for display purposes.
    image_raw = tf.expand_dims(image_raw, 0)
    image_raw = tf.image.resize_images(image_raw, [height, width])
    image_raw = tf.squeeze(image_raw)

    # Batch it up.
    images, images_raw, texts, labels = tf.train.batch(
        [image, image_raw, text, label],
        batch_size=batch_size,
        num_threads=1,
        capacity=2 * batch_size)
    
    return images, images_raw, texts, labels

def get_init_fn(checkpoints_dir, model_name='inception_v1.ckpt'):
    """Returns a function run by the chief worker to warm-start the training.
    """
    checkpoint_exclude_scopes=["InceptionV1/Logits", "InceptionV1/AuxLogits"]
    
    exclusions = [scope.strip() for scope in checkpoint_exclude_scopes]

    variables_to_restore = []
    for var in slim.get_model_variables():
        excluded = False
        for exclusion in exclusions:
            if var.op.name.startswith(exclusion):
                excluded = True
                break
        if not excluded:
            variables_to_restore.append(var)

    return slim.assign_from_checkpoint_fn(
        os.path.join(checkpoints_dir, model_name),
        variables_to_restore)

def fine_tune_model(dataset_dir, checkpoints_dir, train_dir, num_steps):
    """Fine tune the inception model, retraining the last layer.

    Parameters:
        dataset_dir: The directory containing the data.
        checkpoints_dir: The directory contained the pre-trained model.
        train_dir: The directory to save the trained model.
        num_steps: The number of steps training the model.
    """
    if tf.gfile.Exists(train_dir):
        # Delete old model
        tf.gfile.DeleteRecursively(train_dir)
    tf.gfile.MakeDirs(train_dir)

    with tf.Graph().as_default():
        tf.logging.set_verbosity(tf.logging.INFO)
        
        dataset = get_split('train', dataset_dir)
        image_size = inception_v1.default_image_size
        images, _, labels = _load_batch(dataset, height=image_size, width=image_size)

        # Load validation data
        dataset_valid = get_split('validation', dataset_dir)
        images_valid, _, labels_valid = _load_batch(dataset_valid, 
            batch_size=dataset_valid.num_samples, 
            shuffle=False, height=image_size, width=image_size)
        
        # Create the model, use the default arg scope to configure the batch norm parameters.
        with slim.arg_scope(inception_v1.inception_v1_arg_scope()):
            logits, _ = inception_v1.inception_v1(images, num_classes=dataset.num_classes, 
                is_training=True)
            logits_valid, _ = inception_v1.inception_v1(images_valid, 
                num_classes=dataset_valid.num_classes, is_training=False, reuse=True)
            
        # Specify the loss function:
        one_hot_labels = slim.one_hot_encoding(labels, dataset.num_classes)
        slim.losses.softmax_cross_entropy(logits, one_hot_labels)
        total_loss = slim.losses.get_total_loss()

        # Create some summaries to visualize the training process:
        tf.summary.scalar('losses/Total_Loss', total_loss)
      
        # Specify the optimizer and create the train op:
        optimizer = tf.train.AdamOptimizer(learning_rate=1e-5)
        train_op = slim.learning.create_train_op(total_loss, optimizer)

        # Accuracy metrics
        accuracy_valid = slim.metrics.accuracy(tf.cast(labels_valid, tf.int32),
                                               tf.cast(tf.argmax(logits_valid, 1), tf.int32))

        def train_step_fn(session, *args, **kwargs):
            total_loss, should_stop = train_step(session, *args, **kwargs)
            acc_valid = session.run(accuracy_valid)
            sys.stdout.flush()
            train_step_fn.step += 1
            return [total_loss, should_stop]
        
        train_step_fn.step = 0

        # Run the training:
        final_loss = slim.learning.train(
            train_op,
            logdir=train_dir,
            init_fn=get_init_fn(checkpoints_dir),
            train_step_fn=train_step_fn,
            number_of_steps=num_steps)
            
    print('Finished training. Last batch loss {0:.3f}'.format(final_loss))

def fine_tune_model_with_text(dataset_dir, checkpoints_dir, train_dir, num_steps, 
    learning_rate):
    """Fine tune the inception model, retraining the last layer.

    Parameters:
        dataset_dir: The directory containing the data.
        checkpoints_dir: The directory contained the pre-trained model.
        train_dir: The directory to save the trained model.
        num_steps: The number of steps training the model.
    """
    if tf.gfile.Exists(train_dir):
        # Delete old model
        tf.gfile.DeleteRecursively(train_dir)
    tf.gfile.MakeDirs(train_dir)

    with tf.Graph().as_default():
        tf.logging.set_verbosity(tf.logging.INFO)
        
        dataset = get_split_with_text('train', dataset_dir)
        image_size = inception_v1.default_image_size
        images, _, labels = _load_batch(dataset, height=image_size, width=image_size)
        
        # Create the model, use the default arg scope to configure the batch norm parameters.
        with slim.arg_scope(inception_v1.inception_v1_arg_scope()):
            logits, _ = inception_v1.inception_v1(images, num_classes=dataset.num_classes, 
                is_training=True)
            
        # Specify the loss function:
        one_hot_labels = slim.one_hot_encoding(labels, dataset.num_classes)
        slim.losses.softmax_cross_entropy(logits, one_hot_labels)
        total_loss = slim.losses.get_total_loss()

        # Create some summaries to visualize the training process
        # Use tensorboard --logdir=train_dir, careful with pat
        # Different from the logs, because computed on different mini batch of data
        tf.summary.scalar('Loss', total_loss)
      
        # Specify the optimizer and create the train op:
        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
        train_op = slim.learning.create_train_op(total_loss, optimizer)

        def train_step_fn(session, *args, **kwargs):
            total_loss, should_stop = train_step(session, *args, **kwargs)
            #acc_valid = session.run(accuracy_valid)
            sys.stdout.flush()
            train_step_fn.step += 1
            return [total_loss, should_stop]
        
        train_step_fn.step = 0

        # Run the training:
        final_loss = slim.learning.train(
            train_op,
            logdir=train_dir,
            init_fn=get_init_fn(checkpoints_dir),
            save_interval_secs=60,
            save_summaries_secs=60,
            #train_step_fn=train_step_fn,
            number_of_steps=num_steps)
            
    print('Finished training. Last batch loss {0:.3f}'.format(final_loss))

def evaluate_model(checkpoint_dir, log_dir, num_evals):
    """Visualise results with: tensorboard --logdir=logdir.
    
    Parameters:
        checkpoint_dir: Checkpoint of the saved model during training.
        log_dir: Directory to save logs.
        num_evals: Number of batches to evaluate (mean of the batches is displayed).
    """
    
    with tf.Graph().as_default():
        tf.logging.set_verbosity(tf.logging.INFO)

        dataset_dir = 'data'
        # Load train data
        image_size = inception_v1.default_image_size

        dataset_train = get_split_with_text('train', dataset_dir)
        images_train, _, labels_train = _load_batch(dataset_train, batch_size=32, shuffle=False, 
                                                    height=image_size, width=image_size)

        # Create the model, use the default arg scope to configure the batch norm parameters.
        with slim.arg_scope(inception_v1.inception_v1_arg_scope()):
            logits_train, _ = inception_v1.inception_v1(images_train, 
                num_classes=dataset_train.num_classes, 
                                                        is_training=False, reuse=True)
        # Accuracy metrics
        accuracy_train = slim.metrics.streaming_accuracy(tf.cast(labels_train, tf.int32),
            tf.cast(tf.argmax(logits_train, 1), tf.int32))

        # Load validation data
        dataset_valid = get_split_with_text('validation', dataset_dir)
        images_valid, _, labels_valid = _load_batch(dataset_valid, batch_size=32, shuffle=False, 
            height=image_size, width=image_size)

        # Create the model, use the default arg scope to configure the batch norm parameters.
        with slim.arg_scope(inception_v1.inception_v1_arg_scope()):
            logits_valid, _ = inception_v1.inception_v1(images_valid, 
                num_classes=dataset_valid.num_classes, 
                                                        is_training=False, reuse=True)
        # Accuracy metrics
        accuracy_valid = slim.metrics.streaming_accuracy(tf.cast(labels_valid, tf.int32), 
            tf.cast(tf.argmax(logits_valid, 1), tf.int32))

        # Choose the metrics to compute:
        names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({
            'accuracy_train': accuracy_train,
            'accuracy_valid': accuracy_valid,
        })

        for metric_name, metric_value in names_to_values.iteritems():
            tf.summary.scalar(metric_name, metric_value)

        # Evaluate every eval_interval_secs secs or if not specified,
        # every time the checkpoint_dir changes
        slim.evaluation.evaluation_loop(
            '',
            checkpoint_dir,
            log_dir,
            num_evals=num_evals,
            eval_op=names_to_updates.values())

def evaluate_model_2(checkpoint_dir, log_dir, mode, num_evals):
    """Visualise results with: tensorboard --logdir=logdir. 
    
    Parameters:
        checkpoint_dir: Checkpoint of the saved model during training.
        log_dir: Directory to save logs.
        mode: train or validation.
        num_evals: Number of batches to evaluate (mean of the batches is displayed).
    """
    
    with tf.Graph().as_default():
        tf.logging.set_verbosity(tf.logging.INFO)

        dataset_dir = 'data'
        # Load train data
        image_size = inception_v1.default_image_size

        dataset = get_split_with_text(mode, dataset_dir)
        images, _, labels = _load_batch(dataset, batch_size=32, shuffle=False, 
                                        height=image_size, width=image_size)

        # Create the model, use the default arg scope to configure the batch norm parameters.
        with slim.arg_scope(inception_v1.inception_v1_arg_scope()):
            logits, _ = inception_v1.inception_v1(images, num_classes=dataset.num_classes, 
                                                  is_training=False, reuse=True)
        # Accuracy metrics
        accuracy = slim.metrics.streaming_accuracy(tf.cast(labels, tf.int32),
                                                   tf.cast(tf.argmax(logits, 1), tf.int32))

        # Choose the metrics to compute:
        names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({
            'accuracy': accuracy,
        })

        for metric_name, metric_value in names_to_values.iteritems():
            tf.summary.scalar(metric_name, metric_value)

        log_dir = os.path.join(log_dir, mode)

        # Evaluate every eval_interval_secs secs or if not specified,
        # every time the checkpoint_dir changes
        slim.evaluation.evaluation_loop(
            '',
            checkpoint_dir,
            log_dir,
            num_evals=num_evals,
            eval_op=names_to_updates.values())
        
def softmax_regression(num_valid, C):
    """Run a softmax regression on the images.

    Parameters:
        num_valid: Size of the validation set.
        C: Inverse of the regularization strength.
    """
    # Load data
    X_train, X_valid, y_train, y_valid = get_numpy_data('data', num_valid)
    logistic = LogisticRegression(multi_class='multinomial', solver='newton-cg',
                                  C=C, random_state=_RANDOM_SEED)
    print('Start training Logistic Regression.')
    logistic.fit(X_train, y_train)

    accuracy_train = accuracy_score(logistic.predict(X_train), y_train)
    valid_accuracy = accuracy_score(logistic.predict(X_valid), y_valid)
    print('Training accuracy: {0:.3f}'.format(accuracy_train))
    print('Validation accuracy: {0:.3f}'.format(valid_accuracy))

def forest(num_valid, n_estimators, max_depth):
    """Run a Random Forest on the images.

    Parameters:
        num_valid: Size of the validation set.
        n_estimators: Number of trees.
        max_depth: Maximum depth of a tree.
    """
    # Load data
    X_train, X_valid, y_train, y_valid = get_numpy_data('data', num_valid)
    forest = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, 
                                    random_state=_RANDOM_SEED)
    print('Start training Random Forest.')
    forest.fit(X_train, y_train)

    accuracy_train = accuracy_score(forest.predict(X_train), y_train)
    valid_accuracy = accuracy_score(forest.predict(X_valid), y_valid)
    print('Training accuracy: {0:.3f}'.format(accuracy_train))
    print('Validation accuracy: {0:.3f}'.format(valid_accuracy))
\end{lstlisting}

\newpage
\section{Natural Language Processing}

\subsection{Text Preprocessing}
\begin{lstlisting}
import os
import re
import gensim

import numpy as np
import pandas as pd

# string.punctuation
_PUNCTUATION = u'!"$%&\'()*+,./:;<=>?[\\]^_`{|}~#'

_MIN_ENGLISH_WORDS_IN_POST = 5

def _load_embedding_weights_glove(text_dir, emb_dir, filename):
    """Load the word embedding weights from a pre-trained model.
    
    Parameters:
        text_dir: The directory containing the text model.
        emb_dir: The subdirectory containing the weights.
        filename: The name of that text file.
        
    Returns:
        vocabulary: A list containing the words in the vocabulary.
        embedding: A numpy array of the weights.
    """
    vocabulary = []
    embedding = []
    with open(os.path.join(text_dir, emb_dir, filename), 'rb') as f:
        for line in f.readlines():
            row = line.strip().split(' ')
            # Convert to unicode
            vocabulary.append(row[0].decode('utf-8', 'ignore'))
            embedding.append(map(np.float32, row[1:]))
        embedding = np.array(embedding)
        print('Finished loading word embedding weights.')
    return vocabulary, embedding

def _load_embedding_weights_word2vec(text_dir, emb_dir, filename):
    """Load the word embedding weights from a pre-trained model.
    
    Parameters:
        text_dir: The directory containing the text model.
        emb_dir: The subdirectory containing the weights.
        filename: The name of the binary file.
        
    Returns:
        vocabulary: A list containing the words in the vocabulary.
        embedding: A numpy array of the weights.
    """
    word2vec_dir = os.path.join(text_dir, emb_dir, filename)
    model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_dir, binary=True)
    vocabulary = model.index2word
    embedding = model.syn0
    print('Finished loading word embedding weights.')
    return vocabulary, embedding

def _str_list_to_set(str_list):
    """Convert a string representation of a list such as '[happy, sun, outdoors]'
       to a set of strings {'happy', 'sun', 'outdoors'}
    """
    output = str_list[1:-1].split(',')
    output = set([x.strip() for x in output])
    return output

def _df_with_hashtag_in_post(df, tag):
    """Make sure that the relevant hashtag is in the post.
    """
    df['tags'] = df['tags'].map(_str_list_to_set)
    mask = df['tags'].map(lambda x: tag in x)
    return df.loc[mask, :].reset_index(drop=True)

def _is_valid_text(paragraph, vocab_set):
    """Check that a post contains atleast _MIN_ENGLISH_WORDS_IN_POST words in english.
    """
    # Check for nan text
    if (type(paragraph) == float) and (np.isnan(paragraph)):
        return False
    else:
        regex = re.compile('[%s]' % re.escape(_PUNCTUATION))
        # Remove punctuation, convert to lower case before splitting
        words = regex.sub('', paragraph).lower().split()
        # Check if there are atleast _MIN_ENGLISH_WORDS_IN_POST words in english
        return len(set(words).intersection(vocab_set)) > _MIN_ENGLISH_WORDS_IN_POST

def _paragraph_to_ids(paragraph, word_to_id, post_size, emotions):
    """Convert a paragraph to a list of ids, removing the #emotion.
    """
    words = []
    vocab_size = len(word_to_id)

    # Remove emotion hashtags from the post.
    emotion_regex = re.compile('|'.join(map(re.escape, ['#' + emotion for emotion in emotions])))
    paragraph = emotion_regex.sub('', paragraph.lower())

    regex = re.compile('[%s]' % re.escape(_PUNCTUATION))
    # Remove punctuation, convert to lower case before splitting
    words = regex.sub('', paragraph).lower().split()
    # Replace unknown words by an id equal to the size of the vocab
    words = map(lambda x: word_to_id.get(x, vocab_size), words)
        
    if len(words) > post_size:
        words = words[:post_size]
    else:
        words = words + [vocab_size] * (post_size - len(words))
    return words

def preprocess_df(text_dir, emb_dir, filename, emb_name, emotions, post_size):
    """Preprocess emotion dataframes.
    """
    if emb_name == 'word2vec':
        vocabulary, embedding = _load_embedding_weights_word2vec(text_dir, emb_dir, filename)
    else:
        vocabulary, embedding = _load_embedding_weights_glove(text_dir, emb_dir, filename)
    vocab_size, embedding_dim = embedding.shape
    word_to_id = dict(zip(vocabulary, range(vocab_size)))
    # Unknown words = vector with zeros
    embedding = np.concatenate([embedding, np.zeros((1, embedding_dim))])

    columns = ['id', 'post_url', 'type', 'timestamp', 'date', 'tags', 'liked',
               'note_count', 'photo', 'text', 'search_query']
    df_all = pd.DataFrame(columns=columns)
    for emotion in emotions:
        path = os.path.join('data', emotion + '.csv')
        df_emotion = _df_with_hashtag_in_post(pd.read_csv(path, encoding='utf-8'), emotion)
        df_all = pd.concat([df_all, df_emotion]).reset_index(drop=True)

    vocab_set = set(vocabulary)
    mask = df_all['text'].map(lambda x: _is_valid_text(x, vocab_set))
    df_all =  df_all.loc[mask, :].reset_index(drop=True)

    # Map text to ids
    df_all['text_list'] = df_all['text'].map(lambda x: _paragraph_to_ids(x, word_to_id, 
        post_size, emotions))

    # Binarise emotions
    emotion_dict = dict(zip(emotions, range(len(emotions))))
    df_all['search_query'] =  df_all['search_query'].map(emotion_dict)

    # Add <ukn> word to dictionary
    word_to_id['<ukn>'] = vocab_size
    print('Finished loading dataframes.')

    return df_all, word_to_id, embedding

def preprocess_one_df(vocabulary, embedding, emotion, post_size):
    """Preprocess one dataframe for the image/text model.
    """
    vocab_size, embedding_dim = embedding.shape
    word_to_id = dict(zip(vocabulary, range(vocab_size)))
    # Unknown words = vector with zeros
    #embedding = np.concatenate([embedding, np.zeros((1, embedding_dim))])

    path = os.path.join('data', emotion + '.csv')
    df_emotion = _df_with_hashtag_in_post(pd.read_csv(path, encoding='utf-8'), emotion)

    vocab_set = set(vocabulary)
    mask = df_emotion['text'].map(lambda x: _is_valid_text(x, vocab_set))
    df_emotion =  df_emotion.loc[mask, :].reset_index(drop=True)

    emotions = ['happy', 'sad', 'angry', 'scared', 'disgusted', 'surprised']
    # Map text to ids
    df_emotion['text_list'] = df_emotion['text'].map(lambda x: _paragraph_to_ids(x, word_to_id, 
        post_size, emotions))

    # Binarise emotions
    #emotion_dict = dict(zip(emotions, range(len(emotions))))
    #df_all['search_query'] =  df_all['search_query'].map(emotion_dict)

    # Add <ukn> word to dictionary
    #word_to_id['<ukn>'] = vocab_size

    return df_emotion#, word_to_id, embedding
\end{lstlisting}

\subsection{Text Model}
\begin{lstlisting}
import tensorflow as tf
import numpy as np

from time import time
from sklearn.model_selection import train_test_split
from text_model.text_preprocessing import preprocess_df

_RANDOM_SEED = 0

class CharModel():
    def __init__(self, config):
        self.config = config
        vocab_size = config['vocab_size']
        embedding_dim = config['embedding_dim']
        post_size = config['post_size']
        fc1_size = config['fc1_size']
        nb_emotions = config['nb_emotions']
        dropout = config['dropout']
        max_grad_norm = config['max_grad_norm']
        initial_lr = config['initial_lr']
        
        self.input_data = tf.placeholder(tf.int32, [None, post_size])
        self.target = tf.placeholder(tf.int32, [None])
        self.learning_rate = tf.Variable(initial_lr, trainable=False)
        # Use a placeholder to turn off dropout during testing 
        self.keep_prob = tf.placeholder(tf.float32)
        # Placeholder for embedding weights
        self.embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])
        
        # Word embedding
        W_embedding = tf.get_variable('W_embedding', [vocab_size, embedding_dim], 
            trainable=False)
        self.embedding_init = W_embedding.assign(self.embedding_placeholder)
        input_embed = tf.nn.embedding_lookup(W_embedding, self.input_data)
        input_embed_dropout = tf.nn.dropout(input_embed, self.keep_prob)

        # Rescale the mean by the actual number of non-zero values.
        nb_finite = tf.reduce_sum(tf.cast(tf.not_equal(input_embed_dropout, 0.0), tf.float32), 
            axis=1)
        # If a post has zero finite elements, replace nb_finite by 1
        nb_finite = tf.where(tf.equal(nb_finite, 0.0), tf.ones_like(nb_finite), nb_finite)
        self.h1 = tf.reduce_mean(input_embed_dropout, axis=1) * post_size / nb_finite

        # Fully connected layer
        W_fc1 = tf.get_variable('W_fc1', [embedding_dim, fc1_size])
        b_fc1 = tf.get_variable('b_fc1', [fc1_size])
        h2 = tf.matmul(self.h1, W_fc1) + b_fc1
        h2 = tf.nn.relu(h2)

        W_softmax = tf.get_variable('W_softmax', [fc1_size, nb_emotions])
        b_softmax = tf.get_variable('b_softmax', [nb_emotions])
        logits = tf.matmul(h2, W_softmax) + b_softmax
        labels = tf.one_hot(self.target, nb_emotions)
        # Cross-entropy loss
        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, 
            logits=logits))
        # Add to tensorboard
        tf.summary.scalar('Loss', self.loss)

        # Use gradient cliping
        trainable_vars = tf.trainable_variables()
        grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, trainable_vars), 
            max_grad_norm)
        optimizer = tf.train.AdamOptimizer(self.learning_rate)
        self.train_step = optimizer.apply_gradients(zip(grads, trainable_vars),
            global_step=tf.contrib.framework.get_or_create_global_step())
        #self.sample = tf.multinomial(tf.reshape(logits, [-1, vocab_size]), 1)
        correct_pred = tf.equal(tf.cast(tf.argmax(logits, 1), tf.int32), self.target)
        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))

        # Merge summaries
        self.merged = tf.summary.merge_all()

def _shuffling(X, y):
    p = np.random.permutation(X.shape[0])
    return X[p], y[p]

def run_model(sess, model, X, y, is_training, model_gen=None):
    batch_size = model.config['batch_size']
    dropout = model.config['dropout']
    initial_lr = model.config['initial_lr']
    lr_decay = model.config['lr_decay']
    max_epoch_no_decay = model.config['max_epoch_no_decay']
    nb_epochs = model.config['nb_epochs']
    
    nb_batches = X.shape[0] / batch_size
    if is_training:
        # Iteration to print at
        print_iter = list(np.linspace(0, nb_batches - 1, 11).astype(int))
        dropout_param = dropout
        ops = [model.merged, model.loss, model.accuracy, model.train_step]
    else:
        dropout_param = 1.0
        ops = [tf.no_op(), model.loss, model.accuracy, tf.no_op()]

    # Tensorboard writer
    if is_training:
        train_writer = tf.summary.FileWriter('text_model/loss', sess.graph)

    for e in range(nb_epochs):
        print ('Epoch: {0}'.format(e + 1))
        lr_decay = lr_decay ** max(e + 1 - max_epoch_no_decay, 0)
        # would be better to use a placeholder to assign. Here we're modifying the graph.
        sess.run(tf.assign(model.learning_rate, initial_lr * lr_decay))

        total_loss = 0.0
        total_accuracy = 0.0
        nb_iter = 0.0
        loss_history = []
        t0 = time()
        X, y = _shuffling(X, y)
        X_reshaped = X[: (nb_batches * batch_size), :].reshape((nb_batches, batch_size, -1))
        y_reshaped = y[: (nb_batches * batch_size)].reshape((nb_batches, batch_size))
        for i in range(nb_batches):
            curr_input = X_reshaped[i, :, :]
            curr_target = y_reshaped[i, :]
            summary, curr_loss, curr_acc, _ = sess.run(ops, feed_dict=
                {model.input_data: curr_input, 
                 model.target: curr_target,
                 model.keep_prob: dropout_param})
            if is_training:
                train_writer.add_summary(summary, i + e * nb_batches)

            total_loss += curr_loss
            total_accuracy += curr_acc
            nb_iter += 1
            loss_history.append(curr_loss)

            if (is_training and i in print_iter):
                print('{0:.0f}%  loss = {1:.3f}, accuracy = {2:.3f}, speed = {3:.0f} pps'\
                      .format(print_iter.index(i) * 10, 
                              total_loss / nb_iter, total_accuracy / nb_iter,
                              (nb_iter * batch_size) / (time() - t0)))
                
        if is_training:
            pass
            #first_char = np.array([[4]])
            #samples = generate_chars(sess, model_gen, first_char, 2000)
            #generated_chars = map(lambda x: model_gen.config['id_to_char'][x], samples)
            #np.save('generated_chars.npy', np.array(generated_chars))
            #generated_chars = np.load('generated_chars.npy')
            #print('Generated characters:')
            # Need to add encode('utf-8') because when using the server,
            # sys.stdout.encoding is None
            #print(u''.join(list(generated_chars)).replace(u'_', u' ').encode('utf-8'))
        else:
            print('Loss = {0:.3f}, accuracy = {1:.3f}, speed = {2:.0f} pps'\
                  .format(total_loss / nb_iter, total_accuracy / nb_iter,
                          (nb_iter * batch_size) / (time() - t0)))

        #if (is_training and show_loss_graph):
            #plt.plot(perplexity_history)
            #plt.grid(True)
            #plt.title('Epoch {0}'.format(e + 1))
            #plt.xlabel('Mini-batch number')
            #plt.ylabel('Perplexity per mini-batch')
            #plt.show()
            
def generate_chars(sess, model, first_char, max_iteration):
    ops = [model.final_state, model.sample]
    current_char = first_char.copy()
    numpy_state = sess.run(model.initial_state)
    samples = []
    for i in range(max_iteration):
        # Sample from the multinomial distribution of the next character
        numpy_state, sample = sess.run(ops, feed_dict={model.input_data: current_char,
                                                       model.initial_state: numpy_state,
                                                       model.keep_prob: 1.0})
        samples.append(sample[0][0])
        current_char = sample
    return samples

def compute_sklearn_features():
    """Compute mean word embedding features for sklearn models.
    """
    text_dir = 'text_model'
    emb_dir = 'embedding_weights'
    filename = 'glove.6B.50d.txt'
    emb_name = 'glove'
    emotions = ['happy', 'sad', 'angry', 'scared', 'disgusted', 'surprised']
    post_size = 200
    df_all, word_to_id, embedding = preprocess_df(text_dir, emb_dir, filename, emb_name, 
        emotions, post_size)

    X = np.stack(df_all['text_list'])
    y = df_all['search_query'].values

    id_to_word = {i: k for k, i in word_to_id.iteritems()}
    config = {'word_to_id': word_to_id,
              'id_to_word': id_to_word,
              'batch_size': 128,
              'vocab_size': len(word_to_id),
              'embedding_dim': embedding.shape[1],
              'post_size': post_size,
              'fc1_size': 16,
              'nb_emotions': len(emotions),
              'dropout': 1.0, # Proba to keep neurons
              'max_grad_norm': 5.0, # Maximum norm of gradient
              'init_scale': 0.1, # Weights initialization scale
              'initial_lr': 1e-3,
              'lr_decay': 0.5,
              'max_epoch_no_decay': 2, # Number of epochs without decaying learning rate
              'nb_epochs': 10} # Maximum number of epochs
    
    tf.reset_default_graph()
    with tf.Session() as sess:
        print('Computing sklearn features:')
        init_scale = config['init_scale']
        initializer = tf.random_uniform_initializer(-init_scale, init_scale)    
        with tf.variable_scope('Model', reuse=None, initializer=initializer):
            config['nb_epochs'] = 1
            m_train = CharModel(config)
        sess.run(tf.global_variables_initializer())
        sess.run(m_train.embedding_init, feed_dict={m_train.embedding_placeholder: embedding})

        batch_size = m_train.config['batch_size']
        initial_lr = m_train.config['initial_lr']
        
        nb_batches = X.shape[0] / batch_size
        dropout_param = 1.0
        ops = m_train.h1
        
        sess.run(tf.assign(m_train.learning_rate, initial_lr))

        X, y = _shuffling(X, y)
        X_reshaped = X[: (nb_batches * batch_size), :].reshape((nb_batches, batch_size, -1))
        y_reshaped = y[: (nb_batches * batch_size)].reshape((nb_batches, batch_size))
        h1_list = []
        for i in range(nb_batches):
            curr_input = X_reshaped[i, :, :]
            curr_target = y_reshaped[i, :]
            h1_features = sess.run(ops, feed_dict={m_train.input_data: curr_input, 
                                                   m_train.target: curr_target,
                                                   m_train.keep_prob: dropout_param})
            h1_list.append(h1_features)

        X_sklearn = np.vstack(h1_list)
        y_sklearn = y_reshaped.reshape((-1))
        print('Finished')
        return X_sklearn, y_sklearn

def main_text():
    text_dir = 'text_model'
    emb_dir = 'embedding_weights'
    filename = 'glove.6B.50d.txt'
    emb_name = 'glove'
    emotions = ['happy', 'sad', 'angry', 'scared', 'disgusted', 'surprised']
    post_size = 200
    df_all, word_to_id, embedding = preprocess_df(text_dir, emb_dir, filename, emb_name, 
        emotions, post_size)

    X = np.stack(df_all['text_list'])
    y = df_all['search_query'].values
    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, 
        random_state=_RANDOM_SEED)

    id_to_word = {i: k for k, i in word_to_id.iteritems()}
    config = {'word_to_id': word_to_id,
              'id_to_word': id_to_word,
              'batch_size': 128,
              'vocab_size': len(word_to_id),
              'embedding_dim': embedding.shape[1],
              'post_size': post_size,
              'fc1_size': 2048,
              'nb_emotions': len(emotions),
              'dropout': 1.0, # Proba to keep neurons
              'max_grad_norm': 5.0, # Maximum norm of gradient
              'init_scale': 0.1, # Weights initialization scale
              'initial_lr': 1e-3,
              'lr_decay': 0.5,
              'max_epoch_no_decay': 2, # Number of epochs without decaying learning rate
              'nb_epochs': 10} # Maximum number of epochs
    
    tf.reset_default_graph()
    with tf.Session() as sess:
        print('Training:')
        init_scale = config['init_scale']
        initializer = tf.random_uniform_initializer(-init_scale, init_scale)    
        with tf.variable_scope('Model', reuse=None, initializer=initializer):
            config['nb_epochs'] = 5
            m_train = CharModel(config)
        sess.run(tf.global_variables_initializer())
        sess.run(m_train.embedding_init, feed_dict={m_train.embedding_placeholder: embedding})
        # Characters generation
        #with tf.variable_scope('Model', reuse=True):
            #config_gen = dict(config)
            #config_gen['batch_size'] = 1
            #config_gen['num_steps'] = 1
            #m_gen = CharModel(config_gen)
        run_model(sess, m_train, X_train, y_train, is_training=True)
        
        print('\nValidation:')
        with tf.variable_scope('Model', reuse=True):
            config['nb_epochs'] = 1
            m_valid = CharModel(config)
        run_model(sess, m_valid, X_valid, y_valid, is_training=False)
        
        #print('\nTest:')
        #with tf.variable_scope('Model', reuse=True):
         #   m_test =  CharModel(config)
        #run_model(sess, m_test, test_data, is_training=False)
        print('Finished')
\end{lstlisting}

\section{Deep Sentiment}

\begin{lstlisting}
import os
import sys

import numpy as np
import tensorflow as tf

from tensorflow.contrib import slim
from tensorflow.contrib.slim.python.slim.learning import train_step
from tensorflow.python.training import monitored_session
from tensorflow.python.training import saver as tf_saver
from scipy.ndimage.filters import gaussian_filter1d

from slim.preprocessing import inception_preprocessing
from image_model import inception_v1
from datasets import dataset_utils
from text_model.text_preprocessing import _load_embedding_weights_glove
from image_model.im_model import load_batch_with_text, get_init_fn
from datasets.convert_to_dataset import get_split_with_text
import matplotlib.pyplot as plt

_POST_SIZE = 200

def train_deep_sentiment(dataset_dir, checkpoints_dir, train_dir, num_steps, initial_lr):
    """Fine tune the inception model, retraining the last layer.

    Parameters:
        dataset_dir: The directory containing the data.
        checkpoints_dir: The directory contained the pre-trained model.
        train_dir: The directory to save the trained model.
        num_steps: The number of steps training the model.
    """
    if tf.gfile.Exists(train_dir):
        # Delete old model
        tf.gfile.DeleteRecursively(train_dir)
    tf.gfile.MakeDirs(train_dir)

    with tf.Graph().as_default():
        tf.logging.set_verbosity(tf.logging.INFO)

        learning_rate = tf.Variable(initial_lr, trainable=False)
        lr_rate_placeholder = tf.placeholder(tf.float32)
        lr_rate_assign = learning_rate.assign(lr_rate_placeholder)

        dataset = get_split_with_text('train', dataset_dir)
        image_size = inception_v1.default_image_size
        images, _, texts, labels = load_batch_with_text(dataset, height=image_size, 
             width=image_size)
        
        im_features_size = 128
        # Create the model, use the default arg scope to configure the batch norm parameters.
        with slim.arg_scope(inception_v1.inception_v1_arg_scope()):
            images_features, _ = inception_v1.inception_v1(images, num_classes=im_features_size, 
                 is_training=True)

        # Text model
        text_dir = 'text_model'
        emb_dir = 'embedding_weights'
        filename = 'glove.6B.50d.txt'
        vocabulary, embedding = _load_embedding_weights_glove(text_dir, emb_dir, filename)
        vocab_size, embedding_dim = embedding.shape
        word_to_id = dict(zip(vocabulary, range(vocab_size)))
        # Unknown words = vector with zeros
        embedding = np.concatenate([embedding, np.zeros((1, embedding_dim))])
        word_to_id['<ukn>'] = vocab_size

        vocab_size = len(word_to_id)
        nb_emotions = dataset.num_classes
        with tf.variable_scope('Text'):
            embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])
        
            # Word embedding
            W_embedding = tf.get_variable('W_embedding', [vocab_size, embedding_dim], 
                trainable=False)
            embedding_init = W_embedding.assign(embedding_placeholder)
            input_embed = tf.nn.embedding_lookup(W_embedding, texts)
            #input_embed_dropout = tf.nn.dropout(input_embed, self.keep_prob)

            # Rescale the mean by the actual number of non-zero values.
            nb_finite = tf.reduce_sum(tf.cast(tf.not_equal(input_embed, 0.0), tf.float32), axis=1)
            # If a post has zero finite elements, replace nb_finite by 1
            nb_finite = tf.where(tf.equal(nb_finite, 0.0), tf.ones_like(nb_finite), nb_finite)
            h1 = tf.reduce_mean(input_embed, axis=1) * _POST_SIZE / nb_finite

            fc1_size = 2048
            # Fully connected layer
            W_fc1 = tf.get_variable('W_fc1', [embedding_dim, fc1_size])
            b_fc1 = tf.get_variable('b_fc1', [fc1_size])
            texts_features = tf.matmul(h1, W_fc1) + b_fc1
            texts_features = tf.nn.relu(texts_features)

        # Concatenate image and text features
        concat_features = tf.concat([images_features, texts_features], axis=1)

        # Fully connected layer

        W_softmax = tf.get_variable('W_softmax', [im_features_size + fc1_size, nb_emotions])
        b_softmax = tf.get_variable('b_softmax', [nb_emotions])
        logits = tf.matmul(concat_features, W_softmax) + b_softmax
        # Specify the loss function:
        one_hot_labels = slim.one_hot_encoding(labels, nb_emotions)
        slim.losses.softmax_cross_entropy(logits, one_hot_labels)
        total_loss = slim.losses.get_total_loss()

        # Create some summaries to visualize the training process
        # Use tensorboard --logdir=train_dir
        # Different from the logs, because computed on different mini batch of data
        tf.summary.scalar('Loss', total_loss)
      
        # Specify the optimizer and create the train op:
        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
        train_op = slim.learning.create_train_op(total_loss, optimizer)

        nb_batches = dataset.num_samples / 32
        def train_step_fn(session, *args, **kwargs):
            # Decaying learning rate every epoch
            if train_step_fn.step % (nb_batches) == 0:
                lr_decay = 0.5 ** train_step_fn.epoch
                session.run(lr_rate_assign, feed_dict={lr_rate_placeholder: initial_lr * lr_decay})
                print('New learning rate: {0}'. format(initial_lr * lr_decay))
                train_step_fn.epoch += 1

            # Initialise embedding weights
            if train_step_fn.step == 0:
                session.run(embedding_init, feed_dict={embedding_placeholder: embedding})
            total_loss, should_stop = train_step(session, *args, **kwargs)

            #acc_valid = session.run(accuracy_valid)
            #sys.stdout.flush()
            train_step_fn.step += 1
            return [total_loss, should_stop]
        
        train_step_fn.step = 0
        train_step_fn.epoch = 0

        # Run the training:
        final_loss = slim.learning.train(
            train_op,
            logdir=train_dir,
            init_fn=get_init_fn(checkpoints_dir),
            save_interval_secs=60,
            save_summaries_secs=60,
            train_step_fn=train_step_fn,
            number_of_steps=num_steps)
            
    print('Finished training. Last batch loss {0:.3f}'.format(final_loss))

def evaluate_deep_sentiment(checkpoint_dir, log_dir, mode, num_evals):
    """Visualise results with: tensorboard --logdir=logdir.
    
    Parameters:
        checkpoint_dir: Checkpoint of the saved model during training.
        log_dir: Directory to save logs.
        mode: train or validation.
        num_evals: Number of batches to evaluate (mean of the batches is displayed).
    """
    with tf.Graph().as_default():
        tf.logging.set_verbosity(tf.logging.INFO)

        dataset_dir = 'data'
        dataset = get_split_with_text('train', dataset_dir)
        image_size = inception_v1.default_image_size
        images, _, texts, labels = load_batch_with_text(dataset, height=image_size, 
            width=image_size)
        
        im_features_size = 128
        # Create the model, use the default arg scope to configure the batch norm parameters.
        with slim.arg_scope(inception_v1.inception_v1_arg_scope()):
            images_features, _ = inception_v1.inception_v1(images, 
                num_classes=im_features_size, 
                is_training=True)

        # Text model
        text_dir = 'text_model'
        emb_dir = 'embedding_weights'
        filename = 'glove.6B.50d.txt'
        vocabulary, embedding = _load_embedding_weights_glove(text_dir, emb_dir, filename)
        vocab_size, embedding_dim = embedding.shape
        word_to_id = dict(zip(vocabulary, range(vocab_size)))
        # Unknown words = vector with zeros
        embedding = np.concatenate([embedding, np.zeros((1, embedding_dim))])
        word_to_id['<ukn>'] = vocab_size

        vocab_size = len(word_to_id)
        nb_emotions = dataset.num_classes
        with tf.variable_scope('Text'):
            embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])
        
            # Word embedding
            W_embedding = tf.get_variable('W_embedding', [vocab_size, embedding_dim], 
                trainable=False)
            embedding_init = W_embedding.assign(embedding_placeholder)
            input_embed = tf.nn.embedding_lookup(W_embedding, texts)
            #input_embed_dropout = tf.nn.dropout(input_embed, self.keep_prob)

            # Rescale the mean by the actual number of non-zero values.
            nb_finite = tf.reduce_sum(tf.cast(tf.not_equal(input_embed, 0.0), tf.float32), axis=1)
            # If a post has zero finite elements, replace nb_finite by 1
            nb_finite = tf.where(tf.equal(nb_finite, 0.0), tf.ones_like(nb_finite), nb_finite)
            h1 = tf.reduce_mean(input_embed, axis=1) * _POST_SIZE / nb_finite

            fc1_size = 2048
            # Fully connected layer
            W_fc1 = tf.get_variable('W_fc1', [embedding_dim, fc1_size])
            b_fc1 = tf.get_variable('b_fc1', [fc1_size])
            texts_features = tf.matmul(h1, W_fc1) + b_fc1
            texts_features = tf.nn.relu(texts_features)

        # Concatenate image and text features
        concat_features = tf.concat([images_features, texts_features], axis=1)

        W_softmax = tf.get_variable('W_softmax', [im_features_size + fc1_size, nb_emotions])
        b_softmax = tf.get_variable('b_softmax', [nb_emotions])
        logits = tf.matmul(concat_features, W_softmax) + b_softmax

        # Accuracy metrics
        accuracy = slim.metrics.streaming_accuracy(tf.cast(labels, tf.int32),
                                                   tf.cast(tf.argmax(logits, 1), tf.int32))

        # Choose the metrics to compute:
        names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({
            'accuracy': accuracy,
        })

        for metric_name, metric_value in names_to_values.iteritems():
            tf.summary.scalar(metric_name, metric_value)

        log_dir = os.path.join(log_dir, mode)

        # Evaluate every eval_interval_secs secs or if not specified,
        # every time the checkpoint_dir changes
        # tf.get_variable variables are also restored
        slim.evaluation.evaluation_loop(
            '',
            checkpoint_dir,
            log_dir,
            num_evals=num_evals,
            eval_op=names_to_updates.values())
            
def deprocess_image(np_image):
    return (np_image - 0.5) / 2.0

def blur_image(np_image, sigma=1):
    np_image = gaussian_filter1d(np_image, sigma, axis=1)
    np_image = gaussian_filter1d(np_image, sigma, axis=2)
    return np_image

def class_visualisation(label, learning_rate, checkpoint_dir):
    """Visualise class with gradient ascent.
    
    Parameters:
        label: Label to visualise.
        learning_rate: Learning rate of the gradient ascent.
        checkpoint_dir: Checkpoint of the saved model during training.
    """
    with tf.Graph().as_default():
        tf.logging.set_verbosity(tf.logging.INFO)

        image_size = inception_v1.default_image_size
        image = tf.placeholder(tf.float32, [1, image_size, image_size, 3])

        # Text model
        text_dir = 'text_model'
        emb_dir = 'embedding_weights'
        filename = 'glove.6B.50d.txt'
        vocabulary, embedding = _load_embedding_weights_glove(text_dir, 
            emb_dir, filename)
        vocab_size, embedding_dim = embedding.shape
        word_to_id = dict(zip(vocabulary, range(vocab_size)))

        # Create text with only unknown words
        text = tf.constant(np.ones((1, _POST_SIZE), dtype=np.int32) * vocab_size)

        im_features_size = 128
        # Create the model, use the default arg scope to configure the batch norm parameters.
        with slim.arg_scope(inception_v1.inception_v1_arg_scope()):
            images_features, _ = inception_v1.inception_v1(image, 
                num_classes=im_features_size, is_training=True)

        # Unknown words = vector with zeros
        embedding = np.concatenate([embedding, np.zeros((1, embedding_dim))])
        word_to_id['<ukn>'] = vocab_size

        vocab_size = len(word_to_id)
        nb_emotions = 6
        with tf.variable_scope('Text'):
            embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])
        
            # Word embedding
            W_embedding = tf.get_variable('W_embedding', [vocab_size, embedding_dim], 
                trainable=False)
            embedding_init = W_embedding.assign(embedding_placeholder)
            input_embed = tf.nn.embedding_lookup(W_embedding, text)
            #input_embed_dropout = tf.nn.dropout(input_embed, self.keep_prob)

            # Rescale the mean by the actual number of non-zero values.
            nb_finite = tf.reduce_sum(tf.cast(tf.not_equal(input_embed, 0.0), tf.float32), axis=1)
            # If a post has zero finite elements, replace nb_finite by 1
            nb_finite = tf.where(tf.equal(nb_finite, 0.0), tf.ones_like(nb_finite), nb_finite)
            h1 = tf.reduce_mean(input_embed, axis=1) * _POST_SIZE / nb_finite

            fc1_size = 2048
            # Fully connected layer
            W_fc1 = tf.get_variable('W_fc1', [embedding_dim, fc1_size])
            b_fc1 = tf.get_variable('b_fc1', [fc1_size])
            texts_features = tf.matmul(h1, W_fc1) + b_fc1
            texts_features = tf.nn.relu(texts_features)

        # Concatenate image and text features
        concat_features = tf.concat([images_features, texts_features], axis=1)

        W_softmax = tf.get_variable('W_softmax', [im_features_size + fc1_size, nb_emotions])
        b_softmax = tf.get_variable('b_softmax', [nb_emotions])
        logits = tf.matmul(concat_features, W_softmax) + b_softmax

        class_score = logits[:, label]
        l2_reg = 0.001
        regularisation = l2_reg * tf.square(tf.norm(image))
        obj_function = class_score  - regularisation
        grad_obj_function = tf.gradients(obj_function, image)[0]
        grad_normalized = grad_obj_function / tf.norm(grad_obj_function)

        # Initialise image
        image_init = tf.random_normal([image_size, image_size, 3])
        image_init = inception_preprocessing.preprocess_image(image_init, image_size, 
            image_size, is_training=False)
        image_init = tf.expand_dims(image_init, 0)

        # Load model
        checkpoint_path = tf_saver.latest_checkpoint(checkpoint_dir)
        scaffold = monitored_session.Scaffold(
            init_op=None, init_feed_dict=None,
            init_fn=None, saver=None)
        session_creator = monitored_session.ChiefSessionCreator(
            scaffold=scaffold,
            checkpoint_filename_with_path=checkpoint_path,
            master='',
            config=None)

        blur_every = 10
        max_jitter = 16
        show_every = 50
        clip_percentile = 20

        with monitored_session.MonitoredSession(
            session_creator=session_creator, hooks=None) as session:
            np_image = session.run(image_init)
            num_iterations = 500
            for i in range(num_iterations):
                # Randomly jitter the image a bit
                ox, oy = np.random.randint(-max_jitter, max_jitter+1, 2)
                np_image = np.roll(np.roll(np_image, ox, 1), oy, 2)

                # Update image
                grad_update = session.run(grad_normalized, feed_dict={image: np_image})
                np_image += learning_rate * grad_update

                # Undo the jitter
                np_image = np.roll(np.roll(np_image, -ox, 1), -oy, 2)

                # As a regularizer, clip and periodically blur
                #np_image = np.clip(np_image, -0.2, 0.8)
                # Set pixels with small norm to zero
                min_norm = np.percentile(np_image, clip_percentile)
                np_image[np_image < min_norm] = 0.0
                if i % blur_every == 0:
                    np_image = blur_image(np_image, sigma=0.5)

                if i % show_every == 0 or i == (num_iterations - 1):
                    plt.imshow(deprocess_image(np_image[0]))
                    plt.title('Iteration %d / %d' % (i + 1, num_iterations))
                    plt.gcf().set_size_inches(4, 4)
                    plt.axis('off')
                    plt.show()

\end{lstlisting}



