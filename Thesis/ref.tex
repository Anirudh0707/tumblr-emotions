\begin{thebibliography}{999}
\addcontentsline{toc}{chapter}{\numberline{}Bibliography}

\bibitem{seth-twitter}
S. Flaxman and K. Kassam, On \#agony and \#ecstasy: Potential and pitfalls of linguistic sentiment analysis. In preparation, 2016.

\bibitem{bollen}
J. Bollen, H. Mao, X.-J. Zeng, Twitter mood predicts the stock market. In \textit{Journal of Computational Science}, 2011.

\bibitem{ekman}
P. Ekman, An Argument for Basic Emotions. In \textit{Cognitive and Emotion}, 1992.

\bibitem{tumblr-photos}
Tumblr photos:\\
http://fordosjulius.tumblr.com/post/161996729297/just-relax-with-amazing-view-ocean-and\\
http://ybacony.tumblr.com/post/161878010606/on-a-plane-bitchessss-we-about-to-head-out\\
https://little-sleepingkitten.tumblr.com/post/161996340361/its-okay-to-be-upset-its-okay-to-not-always-be\\
http://shydragon327.tumblr.com/post/161929701863/tensions-were-high-this-caturday\\
https://beardytheshank.tumblr.com/post/161087141680/which-tea-peppermint-tea-what-is-your-favorite\\
https://idreamtofflying.tumblr.com/post/161651437343/me-when-i-see-a-couple-expressing-their-affection

\bibitem{huble}
D. H. Huble and T. N. Wiesel, Receptive fields and functional architecture of monkey striate cortex. In \textit{Journal of Physiology (London)}, 1968.

\bibitem{gorner}
Convolution images, M. Gorner, Tensorflow and Deep Learning without a PhD, Presentation at \textit{Google Cloud Next '17}:\\ 
https://docs.google.com/presentation/d/1TVixw6ItiZ8igjp6U17tcgoFrLSaH\\WQmMOwjlgQY9co/pub?slide=id.g1245051c73\_0\_2184\\
The slide on the convolutional neural network was adapted to our architecture.

\bibitem{nair}
V. Nair and G. E. Hinton, Rectified Linear Units Improve Restricted Boltzmann Machines. In \textit{ICML}, 2010.

\bibitem{camb-spark}
Max pooling image, Cambridge Spark:\\
https://cambridgespark.com/content/tutorials/convolutional-neural-networks-with-keras/index.html

\bibitem{alexnet}
A. Krizhevsky, I. Sutskever and G. Hinton, ImageNet Classification with Deep Convolutional
Neural Networks. In \textit{NIPS}, 2012.

\bibitem{googlenet}
C. Szegedy et al., Going deeper with convolutions. In \textit{CVPR}, 2015.

\bibitem{resnet}
K. He et al., Deep Residual Learning for Image Recognition. In \textit{CVPR}, 2016.

\bibitem{transfer}
A. Karpathy, L. Fei-Fei, J. Johnson, Transfer Learning. In \textit{Stanford CS231n Convolutional Neural Networks for Visual Recognition}, 2016.

\bibitem{arora}
S. Arora et al., Provable Bounds for Learning Some Deep Representations. In \textit{ICML}, 2014. 

\bibitem{hebbian}
D. Hebb, in his book The Organization of Behavior, 1949.

\bibitem{inceptionmodule}
Video explaning Inception Module, https://www.youtube.com/watch?v=VxhSouuSZDY.

\bibitem{word2vec}
T. Mikolov et al., Efficient Estimation of Word Representations in Vector Space. In \textit{ICLR}, 2013.

\bibitem{comparison-text}
TensorFlow, Word2Vec tutorial, https://www.tensorflow.org/tutorials/word2vec.

\bibitem{word2vec-architecture}
C. McCormick, Word2Vec Tutorial - The Skip-Gram Model, http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/.

\bibitem{word2vec2}
T. Mikolov et al., Distributed Representations of Words and Phrases and their Compositionality. In \textit{NIPS}, 2013.

\bibitem{nce}
A. Mnih and Y. W. Teh, A fast and simple algorithm for training neural probabilistic language models. In \textit{ICML}, 2012.

\bibitem{nce2}
B. Zoph et al., Simple, Fast Noise-Contrastive Estimation for Large RNN Vocabularies. In {NAACL}, 2016.

\bibitem{googleword2vec}
Word2Vec pre-trained model, Google, 2013. https://code.google.com/archive/p/word2vec/

\bibitem{seth1}
S. Flaxman et al., Who Supported Obama in 2012? Ecological Inference through Distribution Regression. In \textit{KDD}, 2015.

\bibitem{unres-rnn}
A. Karpathy, The Unreasonable Effectiveness of Recurrent Neural Networks, 2015. http://karpathy.github.io/2015/05/21/rnn-effectiveness/

\bibitem{colah}
C. Olah, Understanding LSTM Networks, 2015. http://colah.github.io/posts/2015-08-Understanding-LSTMs/

\end{thebibliography}




