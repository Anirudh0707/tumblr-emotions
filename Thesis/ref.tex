\begin{thebibliography}{999}
\addcontentsline{toc}{chapter}{\numberline{}Bibliography}

\bibitem{seth-twitter}
S. Flaxman and K. Kassam, On \#agony and \#ecstasy: Potential and pitfalls of linguistic sentiment analysis. In preparation, 2016.

\bibitem{bollen}
J. Bollen, H. Mao, X.-J. Zeng, Twitter mood predicts the stock market. In \textit{Journal of Computational Science}, 2011.

\bibitem{googlenet}
C. Szegedy et al., Going deeper with convolutions. In \textit{CVPR}, 2015.

\bibitem{word2vec}
T. Mikolov et al., Efficient Estimation of Word Representations in Vector Space. In \textit{ICLR}, 2013.

\bibitem{ekman}
P. Ekman, An Argument for Basic Emotions. In \textit{Cognitive and Emotion}, 1992.

\bibitem{tumblr-photos}
Tumblr photos.\\
http://fordosjulius.tumblr.com/post/161996729297/just-relax-with-amazing-view-ocean-and\\
http://ybacony.tumblr.com/post/161878010606/on-a-plane-bitchessss-we-about-to-head-out\\
https://little-sleepingkitten.tumblr.com/post/161996340361/its-okay-to-be-upset-its-okay-to-not-always-be\\
http://shydragon327.tumblr.com/post/161929701863/tensions-were-high-this-caturday\\
https://beardytheshank.tumblr.com/post/161087141680/which-tea-peppermint-tea-what-is-your-favorite\\
https://idreamtofflying.tumblr.com/post/161651437343/me-when-i-see-a-couple-expressing-their-affection

\bibitem{hubel}
D. H. Hubel and T. N. Wiesel, Receptive fields and functional architecture of monkey striate cortex. In \textit{Journal of Physiology (London)}, 1968.

\bibitem{gorner}
Convolution images, M. Gorner, Tensorflow and Deep Learning without a PhD. Presentation at \textit{Google Cloud Next}, 2017.\\ 
https://docs.google.com/presentation/d/1TVixw6ItiZ8igjp6U17tcgoFrLSaH\\WQmMOwjlgQY9co/pub?slide=id.g1245051c73\_0\_2184\\
The slide on the convolutional neural network was adapted to our architecture.

\bibitem{zeropadding}
Zero-padding image, A. Deshpande, A Beginner's Guide To Understanding Convolutional Neural Networks Part 2, 2016. \\
https://adeshpande3.github.io/A-Beginner\%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/

\bibitem{nair}
V. Nair and G. E. Hinton, Rectified Linear Units Improve Restricted Boltzmann Machines. In \textit{ICML}, 2010.

\bibitem{camb-spark}
Max pooling image, P. Velickovic, 
Deep learning for complete beginners: convolutional neural networks with keras, 2017.\\
https://cambridgespark.com/content/tutorials/convolutional-neural-networks-with-keras/index.html

\bibitem{alexnet}
A. Krizhevsky, I. Sutskever and G. Hinton, ImageNet Classification with Deep Convolutional
Neural Networks. In \textit{NIPS}, 2012.

\bibitem{resnet}
K. He et al., Deep Residual Learning for Image Recognition. In \textit{CVPR}, 2016.

\bibitem{transfer}
A. Karpathy, L. Fei-Fei, J. Johnson, Transfer Learning. In \textit{Stanford CS231n Convolutional Neural Networks for Visual Recognition}, 2016.

\bibitem{arora}
S. Arora et al., Provable Bounds for Learning Some Deep Representations. In \textit{ICML}, 2014. 

\bibitem{hebbian}
D. Hebb, in his book The Organization of Behavior, 1949.

\bibitem{inceptionmodule}
Video explaning Inception Module, Udacity, 2016. \\ https://www.youtube.com/watch?v=VxhSouuSZDY

\bibitem{comparison-text}
Word2Vec tutorial, Tensorflow, 2017. \\
https://www.tensorflow.org/tutorials/word2vec

\bibitem{word2vec-architecture}
C. McCormick, Word2Vec Tutorial - The Skip-Gram Model, 2016. \\http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/

\bibitem{word2vec2}
T. Mikolov et al., Distributed Representations of Words and Phrases and their Compositionality. In \textit{NIPS}, 2013.

\bibitem{nce}
A. Mnih and Y. W. Teh, A fast and simple algorithm for training neural probabilistic language models. In \textit{ICML}, 2012.

\bibitem{nce2}
B. Zoph et al., Simple, Fast Noise-Contrastive Estimation for Large RNN Vocabularies. In {NAACL}, 2016.

\bibitem{googleword2vec}
Word2Vec pre-trained model, Google, 2013. \\https://code.google.com/archive/p/word2vec/

\bibitem{seth1}
S. Flaxman et al., Who Supported Obama in 2012? Ecological Inference through Distribution Regression. In \textit{KDD}, 2015.

\bibitem{unres-rnn}
A. Karpathy, The Unreasonable Effectiveness of Recurrent Neural Networks, 2015. http://karpathy.github.io/2015/05/21/rnn-effectiveness/

\bibitem{colah}
C. Olah, Understanding LSTM Networks, 2015. \\http://colah.github.io/posts/2015-08-Understanding-LSTMs/

\bibitem{lstm}
S. Hochreiter and J. Schmidhuber, Long Short-Term Memory. In \textit{Neural Computation}, 1997.

\bibitem{difficulty-rnn}
R. Pascanu et al., On the difficulty of training recurrent neural networks. In \textit{ICML}, 2013.

\bibitem{mcgurk}
H. McGurk and J. MacDonald, Hearing lips and seeing voices. In \textit{Nature}, 1976.

\bibitem{class-vis}
K. Simonyan, A. Vedaldi, and A. Zisserman, Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps. In \textit{ICLR}, 2014.

\bibitem{class-vis2}
J. Yosinski et al., Understanding Neural Networks Through Deep Visualization. In \textit{ICML} 2015.

\bibitem{nguyen}
A. Nguyen et al., Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images. In \textit{CVPR}, 2015.

\end{thebibliography}




