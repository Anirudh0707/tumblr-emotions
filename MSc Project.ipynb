{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pytumblr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk.data\n",
    "import logging\n",
    "from gensim.models import word2vec\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from time import time, sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Tumblr data scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Authenticate via OAuth\n",
    "client = pytumblr.TumblrRestClient(\n",
    "  'IfgCzO5vai28fvtoOZqhirNO4Wy4XWDEBAB0iORxfCCxcpqfky',\n",
    "  'c7NCLElazxRkIUgZw00Ur5lW6qsL2SR8qmVdQ3jyd29RkoeGB3',\n",
    "  '4z8Ljx4lc2YJkh5rCf6dEWAGs9A4M0Z7EQFL7jVH4AnDBeCd69',\n",
    "  'JTHBUNxnSoK5VcB7nHipx7rSYbMGilW27DceHVhu9lI0z0Law1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 6 basic emotions: happiness, sadness, anger, disgust, fear, surprise\n",
    "search_query = 'happy'\n",
    "# Posts available here: https://www.tumblr.com/tagged/happy\n",
    "\n",
    "# Angry, disgusted -> no more posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-bf61d51df1b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0melt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtagged\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mcurrent_post\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mcurrent_post\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mcurrent_post\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'post_url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "posts = []\n",
    "# Load previously scraped data\n",
    "df = pd.read_csv('./data/' + search_query + '.csv', encoding='utf-8')\n",
    "# Continue scraping starting from the oldest timestamp\n",
    "before = df['timestamp'].min()\n",
    "\n",
    "# 800 requests to avoid exceeding Tumblr API's limitations\n",
    "for i in range(100):\n",
    "    tagged = client.tagged(search_query, filter='text', before=before)\n",
    "    for elt in tagged:\n",
    "        current_post = []\n",
    "        current_post.append(elt['id'])\n",
    "        current_post.append(elt['post_url'])\n",
    "\n",
    "        elt_type = elt['type']\n",
    "        current_post.append(elt_type)\n",
    "        current_post.append(elt['timestamp'])\n",
    "        current_post.append(elt['date'])\n",
    "        current_post.append(elt['tags'])\n",
    "        current_post.append(elt['liked'])\n",
    "        current_post.append(elt['note_count'])\n",
    "\n",
    "        if (elt_type == 'photo'):\n",
    "            # Only take the first image\n",
    "            current_post.append(elt['photos'][0]['original_size']['url'])\n",
    "            current_post.append(elt['caption'].replace('\\n',' ').replace('\\r',' '))\n",
    "            current_post.append(search_query)\n",
    "            posts.append(current_post)\n",
    "        elif (elt_type == 'text'):\n",
    "            current_post.append(np.nan)\n",
    "            current_post.append(elt['body'].replace('\\n',' ').replace('\\r',' '))\n",
    "            current_post.append(search_query)\n",
    "            posts.append(current_post)\n",
    "            \n",
    "    before = elt['timestamp']\n",
    "    \n",
    "print('The scraping took {0}s'.format(time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_posts = pd.DataFrame(posts, columns=['id', 'post_url', 'type', 'timestamp', 'date',\n",
    "                                        'tags', 'liked', 'note_count', 'photo', 'text', 'search_query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Concatenate and save the new posts\n",
    "df_concat = pd.concat([df, df_posts]).reset_index(drop=True)\n",
    "df_concat.to_csv('./data/' + search_query + '.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert a sentence for a list of words\n",
    "def sentence_to_wordlist(review):\n",
    "    # Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    \n",
    "    # Remove non-letters\n",
    "    review_text = re.sub('[^a-zA-Z]', ' ', review_text)\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    return words\n",
    "\n",
    "# Convert a paragraph to a list of list of words\n",
    "def string_to_sentences(string, tokenizer):\n",
    "    sentences = []\n",
    "    # Check for nan text\n",
    "    if (type(string) == float) and (np.isnan(string)):\n",
    "        pass\n",
    "    else:\n",
    "        raw_sentences = tokenizer.tokenize(string.strip())\n",
    "        for elt in raw_sentences:\n",
    "            if (len(elt) > 0):\n",
    "                sentences.append(sentence_to_wordlist(elt))\n",
    "            \n",
    "    return sentences\n",
    "\n",
    "# Convert a paragraph to a list of words\n",
    "def string_to_words_weights(string, tokenizer, vocab, vocab_weights):\n",
    "    sentences = []\n",
    "    # Check for nan text\n",
    "    if (type(string) == float) and (np.isnan(string)):\n",
    "        pass\n",
    "    else:\n",
    "        raw_sentences = tokenizer.tokenize(string.strip())\n",
    "        for elt in raw_sentences:\n",
    "            if (len(elt) > 0):\n",
    "                sentences.extend(sentence_to_wordlist(elt))\n",
    "\n",
    "    words_weights = []\n",
    "    for elt in sentences:\n",
    "        if elt in vocab:\n",
    "            words_weights.append(vocab_weights[elt])\n",
    "\n",
    "    return np.array(words_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_features(X):\n",
    "    train = X.copy()\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = []\n",
    "\n",
    "    for text in train['text']:\n",
    "        sentences += string_to_sentences(text, tokenizer)\n",
    "        \n",
    "    # Import the built-in logging module and configure it so that Word2Vec \n",
    "    # creates nice output messages\n",
    "    #logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "     #                   level=logging.INFO)\n",
    "\n",
    "    # Set values for various parameters\n",
    "    num_features = 300    # Word vector dimensionality                      \n",
    "    min_word_count = 40   # Minimum word count                        \n",
    "    num_workers = 4       # Number of threads to run in parallel\n",
    "    context = 10          # Context window size                                                                                    \n",
    "    downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "    # Initialize and train the model (this will take some time)\n",
    "    print \"Training model...\"\n",
    "    model = word2vec.Word2Vec(sentences, workers=num_workers,\n",
    "                              size=num_features, min_count=min_word_count,\n",
    "                              window=context, sample=downsampling)\n",
    "\n",
    "    vocab = set(model.wv.vocab.keys())\n",
    "    vocab_weights = model.wv\n",
    "\n",
    "    train['text_list'] = train['text'].map(lambda x: string_to_words_weights(x, tokenizer, vocab, vocab_weights))\n",
    "    mask = train['text_list'].map(len) > 0\n",
    "    train_reduced = train.loc[mask, :].reset_index(drop=True)\n",
    "    return train_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "# Use only 2 emotions first: happiness and sadness\n",
    "df_happy = pd.read_csv('./data/happy.csv', encoding='utf-8')\n",
    "df_sad = pd.read_csv('./data/sad.csv', encoding='utf-8')\n",
    "df_all = pd.concat([df_happy, df_sad]).reset_index(drop=True)\n",
    "train = create_features(df_all)\n",
    "\n",
    "# Binarise emotions\n",
    "emotion_dict = dict(zip(['happy', 'sad'], [1, 0]))\n",
    "train['search_query'] =  train['search_query'].map(emotion_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Random Fourier features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_components = 100\n",
    "random_seed = 8\n",
    "rbf = RBFSampler(gamma=1, n_components=n_components, random_state=random_seed)\n",
    "for i in range(n_components):\n",
    "    train['rbf_feature_' + str(i)] = 0\n",
    "rbf_columns = ['rbf_feature_' + str(i) for i in range(n_components)]\n",
    "train[rbf_columns] = np.vstack(train['text_list'].map(lambda x: rbf.fit_transform(x).mean(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.859248708096\n",
      "Test score: 0.749884062452\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train[rbf_columns], train['search_query'], \n",
    "                                                    test_size=0.3, random_state=random_seed)\n",
    "forest = RandomForestClassifier(max_depth=10)\n",
    "forest.fit(X_train, y_train)\n",
    "y_train_pred = forest.predict(X_train)\n",
    "y_test_pred = forest.predict(X_test)\n",
    "print('Train score: {0}'.format(accuracy_score(y_train_pred, y_train)))\n",
    "print('Test score: {0}'.format(accuracy_score(y_test_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train score: 0.965681727839\n",
    "Test score: 0.744782810326"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "-> Compare with NLTK package"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
